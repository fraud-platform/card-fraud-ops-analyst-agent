# Fraud Analytics Improvements Implementation Plan

**Version:** 1.0
**Date:** 2026-02-15
**Status:** Draft (Requires Cross-Service Coordination)
**Owner:** Ops Analyst Agent Team

---

## Executive Summary

This document details the integration of 7 advanced fraud analytics improvements into the Card Fraud Ops Analyst Agent. These improvements, derived from the analytics-agent codebase, enhance vector similarity search, evidence analysis, narrative generation, explanation building, and temporal weighting.

### Important Implementation Constraints (Repo Guardrails)

- **Do not mutate Transaction Management tables** (e.g., avoid altering `fraud_gov.transactions`). Store new analytics artifacts in **ops-agent-owned tables** and join via `transaction_id`.
- **UUIDs are generated by the application** using `uuid.uuid7()` (Python 3.14). Do not rely on database-generated UUID defaults.
- **Secrets/config are injected via Doppler** at runtime (no `.env` files).
- **Use `uv` for dependencies and tooling** (no `pip`).

### Key Improvements

1. **Vector Similarity Search with pgvector** - Semantic similarity using embeddings
2. **Counter-Evidence Detection** - Identify 3DS success and trusted devices
3. **Conflict Matrix Analysis** - Multi-dimensional evidence conflict resolution
4. **Enhanced Narrative Generation** - Structured storytelling with domain context
5. **Explanation Builder** - Human-readable analysis with section formatting
6. **Freshness Weighting** - Time-decay function for evidence relevance
7. **Structured Evidence Envelope** - Unified evidence storage format

### Business Impact

- **Accuracy (target)**: +15-25% fraud detection precision via vector similarity
- **False Positives (target)**: -30% reduction through counter-evidence detection
- **Analyst Efficiency (target)**: +40% faster reviews with enhanced narratives
- **Risk Coverage (target)**: +20% improved temporal coverage via freshness weighting

### Implementation Timeline

- **Phase 1** (Weeks 1-2): Database schema + pgvector setup
- **Phase 2** (Weeks 3-4): Core engine enhancements
- **Phase 3** (Weeks 5-6): LLM prompt improvements + testing
- **Phase 4** (Weeks 7-8): Integration + feature flag rollout

**Total Effort:** ~8 weeks (2 developer-months)

---

## 1. Current State Analysis

### 1.1 Existing Capabilities

#### Similarity Engine (Stub Implementation)
```python
# app/agents/similarity_engine.py
class SimilarityEngine:
    async def analyze(self, context: dict) -> dict:
        """Currently returns zero score - stub implementation."""
        similar_transactions = []
        result = evaluate_similarity(transaction, similar_transactions)
        return {"similarity_result": result}
```

**Limitations:**
- No actual similar transaction lookup
- No semantic matching
- No vector database integration
- Returns zero score always

#### Reasoning Engine (Basic LLM)
```python
# app/llm/prompts/investigation_v1.py
system_prompt = """You are a senior fraud analyst assistant.
Provide reasoning that connects evidence to conclusions.
Output format: JSON with narrative, risk_assessment, key_findings, confidence."""
```

**Limitations:**
- Minimal prompt structure
- No counter-evidence consideration
- No conflict resolution guidance
- Basic narrative without domain context

#### Evidence Storage (JSONB)
```sql
-- ops_agent_evidence table
evidence_payload JSONB  -- Unstructured storage
```

**Limitations:**
- No standardized schema
- No queryability
- No versioning
- Mixed formats across evidence types

### 1.2 Target Improvements Comparison

| Feature | Current State | Target State | Improvement |
|---------|--------------|--------------|-------------|
| **Similarity** | Stub (score=0) | Vector search with pgvector | **NEW** |
| **Counter-Evidence** | Not detected | 3DS success, trusted devices | **NEW** |
| **Conflict Analysis** | Severity averaging | Multi-dimensional matrix | **NEW** |
| **Narrative** | 2-3 sentence summary | Structured sections | 5x more detail |
| **Explanation** | Not implemented | 5-section formatted output | **NEW** |
| **Freshness** | Binary time buckets | Exponential decay curve | Continuous |
| **Evidence Schema** | Ad-hoc JSONB | Structured envelope | Queryable |

---

## 2. Improvement Specifications

### Improvement 1: Vector Similarity with pgvector

#### Overview
Enable semantic similarity search using sentence embeddings and cosine similarity via pgvector extension.

#### Technical Details

**Model**: `mxbai-embed-large` (MixedBread AI)
- Dimensions: 1024
- Max tokens: 512
- Inference: Local via Ollama or API

**Similarity Metric**: Cosine similarity
```python
score = 1 - (query_vector <=> candidate_vector)
-- pgvector operator <=> returns cosine distance
```

**Database Schema Changes**
```sql
-- Migration: 006_add_similarity_vector.sql
CREATE EXTENSION IF NOT EXISTS vector;

-- NOTE: Do NOT alter `fraud_gov.transactions` (owned by Transaction Management).
-- Store embeddings in an ops-agent-owned table and JOIN by transaction_id.

CREATE TABLE IF NOT EXISTS fraud_gov.ops_agent_transaction_embeddings (
    transaction_id UUID PRIMARY KEY REFERENCES fraud_gov.transactions(id) ON DELETE CASCADE,
    embedding vector(1024) NOT NULL,
    model_name TEXT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_ops_agent_tx_embeddings_embedding
ON fraud_gov.ops_agent_transaction_embeddings
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Similarity matches SHOULD be persisted as structured evidence envelopes
-- (evidence_kind="similarity") in ops_agent_evidence, rather than creating
-- a new ops_agent_* table, unless/until we explicitly expand the owned-table set.
```

#### Implementation

**Core Module**: `app/agents/similarity_engine_core.py`
```python
from dataclasses import dataclass
from typing import Any
import numpy as np

@dataclass(frozen=True)
class SimilarityMatch:
    match_id: str
    match_type: str  # "vector", "attribute", "temporal"
    similarity_score: float
    details: dict[str, Any]
    counter_evidence: list[dict[str, Any]] | None = None

@dataclass(frozen=True)
class SimilarityResult:
    matches: list[SimilarityMatch]
    overall_score: float
    conflict_matrix: dict[str, Any] | None = None
    vector_search_count: int = 0
    attribute_match_count: int = 0

def compute_cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """Compute cosine similarity between two vectors."""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return dot_product / (norm1 * norm2)
```

**DB Module**: `app/agents/similarity_engine.py`
```python
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text
import numpy as np

class SimilarityEngine:
    def __init__(self, session: AsyncSession):
        self.session = session

    async def analyze(self, context: dict[str, Any]) -> dict[str, Any]:
        """Run vector similarity search + attribute matching."""
        transaction = context.get("transaction")

        # 1. Generate embedding for query transaction
        query_embedding = await self._generate_embedding(transaction)

        # 2. Vector search via pgvector
        vector_matches = await self._vector_search(query_embedding, transaction)

        # 3. Attribute-based matching (card, merchant, amount)
        attribute_matches = await self._attribute_search(transaction)

        # 4. Merge and deduplicate
        all_matches = self._merge_matches(vector_matches, attribute_matches)

        # 5. Detect counter-evidence
        for match in all_matches:
            match.counter_evidence = await self._detect_counter_evidence(
                transaction, match
            )

        result = evaluate_similarity(transaction, all_matches)
        return {"similarity_result": result}

    async def _vector_search(
        self, query_embedding: np.ndarray, transaction
    ) -> list[dict]:
        """Search using pgvector cosine similarity."""
        query = text("""
            SELECT
                t.id as transaction_id,
                t.transaction_id as business_key,
                t.card_id,
                t.merchant_id,
                t.transaction_amount,
                t.decision,
                1 - (e.embedding <=> :query_vector::vector) as similarity_score,
                t.transaction_timestamp
            FROM fraud_gov.transactions t
            JOIN fraud_gov.ops_agent_transaction_embeddings e
              ON e.transaction_id = t.id
            WHERE
                AND t.id != :exclude_id
                AND t.transaction_timestamp >= NOW() - INTERVAL '90 days'
            ORDER BY e.embedding <=> :query_vector::vector
            LIMIT 20
        """)

        result = await self.session.execute(query, {
            "query_vector": str(query_embedding.tolist()),
            "exclude_id": transaction.id
        })

        matches = []
        for row in result:
            matches.append({
                "match_id": str(row.transaction_id),
                "match_type": "vector",
                "similarity_score": float(row.similarity_score),
                "details": {
                    "card_id": str(row.card_id),
                    "merchant_id": str(row.merchant_id),
                    "amount": float(row.transaction_amount),
                    "decision": row.decision
                }
            })
        return matches

    async def _attribute_search(self, transaction) -> list[dict]:
        """Search by exact attributes (card, merchant)."""
        query = text("""
            SELECT
                t.id as transaction_id,
                t.transaction_id as business_key,
                t.card_id,
                t.merchant_id,
                t.transaction_amount,
                t.decision,
                t.transaction_timestamp,
                CASE
                    WHEN t.card_id = :card_id AND t.merchant_id = :merchant_id THEN 0.8
                    WHEN t.card_id = :card_id THEN 0.6
                    WHEN t.merchant_id = :merchant_id THEN 0.4
                    ELSE 0.2
                END as similarity_score
            FROM fraud_gov.transactions t
            WHERE
                (t.card_id = :card_id OR t.merchant_id = :merchant_id)
                AND t.id != :exclude_id
                AND t.transaction_timestamp >= NOW() - INTERVAL '90 days'
            ORDER BY similarity_score DESC, t.transaction_timestamp DESC
            LIMIT 10
        """)

        result = await self.session.execute(query, {
            "card_id": str(transaction.card_id),
            "merchant_id": str(transaction.merchant_id),
            "exclude_id": transaction.id
        })

        matches = []
        for row in result:
            matches.append({
                "match_id": str(row.transaction_id),
                "match_type": "attribute",
                "similarity_score": float(row.similarity_score),
                "details": {
                    "card_id": str(row.card_id),
                    "merchant_id": str(row.merchant_id),
                    "amount": float(row.transaction_amount),
                    "decision": row.decision
                }
            })
        return matches
```

**Configuration**: `app/core/config.py`
```python
class VectorSearchConfig(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="VECTOR_")

    enabled: bool = False
    model_name: str = "mxbai-embed-large"
    inference_provider: str = "ollama"  # ollama, openai, cohere
    api_base: str | None = None
    api_key: str | None = None
    dimension: int = 1024
    search_limit: int = 20
    time_window_days: int = 90
    min_similarity: float = 0.3
```

**Doppler Config**:
```bash
# Add to Doppler project
VECTOR_ENABLED=true
VECTOR_MODEL_NAME=mxbai-embed-large
VECTOR_INFERENCE_PROVIDER=ollama
VECTOR_API_BASE=http://localhost:11434/api
VECTOR_DIMENSION=1024
VECTOR_SEARCH_LIMIT=20
VECTOR_TIME_WINDOW_DAYS=90
```

#### Testing
```python
# tests/unit/test_similarity_engine_vector.py
import pytest
import numpy as np

@pytest.mark.asyncio
async def test_vector_search_embedding():
    engine = SimilarityEngine(mock_session)

    # Mock embedding generation
    mock_embedding = np.random.rand(1024)

    # Mock DB response
    mock_session.execute.return_value.fetchall.return_value = [
        Mock(
            transaction_id=uuid4(),
            similarity_score=0.85,
            card_id=uuid4(),
            merchant_id=uuid4()
        )
    ]

    result = await engine._vector_search(mock_embedding, mock_transaction)

    assert len(result) == 1
    assert result[0]["match_type"] == "vector"
    assert result[0]["similarity_score"] >= 0.3

@pytest.mark.asyncio
async def test_cosine_similarity():
    vec1 = np.array([1.0, 0.0, 0.0])
    vec2 = np.array([1.0, 0.0, 0.0])
    vec3 = np.array([0.0, 1.0, 0.0])

    assert compute_cosine_similarity(vec1, vec2) == pytest.approx(1.0)
    assert compute_cosine_similarity(vec1, vec3) == pytest.approx(0.0)
```

---

### Improvement 2: Counter-Evidence Detection

#### Overview
Detect evidence that reduces fraud risk: successful 3DS auth, trusted devices, historical low-risk patterns.

#### Technical Details

**Counter-Evidence Types**

1. **3DS Success**
```python
@dataclass(frozen=True)
class CounterEvidence:
    evidence_type: str  # "3ds_success", "trusted_device", "low_risk_history"
    strength: float  # 0.0-1.0
    description: str
    supporting_data: dict[str, Any]

def detect_3ds_success(transaction: Any, similar_transactions: list) -> CounterEvidence | None:
    """Detect if similar transactions had successful 3DS authentication."""
    three_ds_success = [
        tx for tx in similar_transactions
        if tx.get("three_ds_authenticated") is True
    ]

    if len(three_ds_success) >= 2:
        success_rate = len(three_ds_success) / len(similar_transactions)
        return CounterEvidence(
            evidence_type="3ds_success",
            strength=min(success_rate, 1.0),
            description=f"{len(three_ds_success)} similar transactions had successful 3DS",
            supporting_data={
                "three_ds_count": len(three_ds_success),
                "total_count": len(similar_transactions),
                "success_rate": success_rate
            }
        )
    return None
```

2. **Trusted Device**
```python
def detect_trusted_device(transaction: Any, context: dict) -> CounterEvidence | None:
    """Detect if device has good history."""
    device_fingerprint = context.get("device_fingerprint")

    if not device_fingerprint:
        return None

    # Query transaction history for this device
    history = context.get("device_history", [])

    successful_transactions = [
        tx for tx in history
        if tx.get("decision") == "APPROVE"
    ]

    if len(successful_transactions) >= 5:
        approval_rate = len(successful_transactions) / len(history)
        if approval_rate > 0.9:
            return CounterEvidence(
                evidence_type="trusted_device",
                strength=0.8,
                description=f"Device has {len(successful_transactions)} prior approvals ({approval_rate:.1%})",
                supporting_data={
                    "device_fingerprint": device_fingerprint,
                    "approval_count": len(successful_transactions),
                    "total_count": len(history),
                    "approval_rate": approval_rate
                }
            )
    return None
```

3. **Low-Risk History**
```python
def detect_low_risk_history(transaction: Any) -> CounterEvidence | None:
    """Detect if card/merchant combo has low fraud rate."""
    # Analyze last 90 days
    recent_txs = transaction.get("recent_transactions", [])

    declined_count = sum(1 for tx in recent_txs if tx.get("decision") == "DECLINE")
    total_count = len(recent_txs)

    if total_count >= 10 and declined_count == 0:
        return CounterEvidence(
            evidence_type="low_risk_history",
            strength=0.7,
            description=f"Clean history: {total_count} prior approvals, no declines",
            supporting_data={
                "approval_count": total_count,
                "decline_count": 0,
                "timeframe_days": 90
            }
        )
    return None
```

#### Integration

**Update**: `app/agents/similarity_engine_core.py`
```python
def evaluate_similarity(
    transaction: Any,
    similar_transactions: list[dict[str, Any]],
) -> SimilarityResult:
    """Evaluate similarity and detect counter-evidence."""
    matches = []

    for sim_tx in similar_transactions:
        score = compute_attribute_similarity(transaction, sim_tx)

        # Detect counter-evidence for this match
        counter_evidence_list = []
        if sim_tx.get("three_ds_authenticated"):
            counter_evidence_list.append({
                "type": "3ds_success",
                "strength": 0.8,
                "description": "Similar transaction had successful 3DS"
            })

        if score > 0:
            matches.append(
                SimilarityMatch(
                    match_id=sim_tx["transaction_id"],
                    match_type="attribute",
                    similarity_score=score,
                    details=sim_tx,
                    counter_evidence=counter_evidence_list if counter_evidence_list else None
                )
            )

    # Compute overall score with counter-evidence discount
    overall = compute_overall_score_with_discount(matches)

    return SimilarityResult(matches=matches, overall_score=overall)
```

---

### Improvement 3: Conflict Matrix Analysis

#### Overview
Multi-dimensional conflict resolution across evidence types (patterns, similarity, counter-evidence, LLM assessment).

#### Technical Details

**Conflict Dimensions**

1. **Pattern vs Similarity**
   - High pattern score + Low similarity = Pattern-driven fraud
   - Low pattern score + High similarity = Novel pattern

2. **Fraud vs Counter-Evidence**
   - Strong fraud signals + Strong counter-evidence = CONFLICT
   - Resolution: Time order (which came first?)

3. **Deterministic vs LLM**
   - Severity mismatch = Flag for human review
   - Resolution: Trust deterministic if confidence < 0.7

**Conflict Matrix Schema**
```python
@dataclass(frozen=True)
class ConflictMatrix:
    """Multi-dimensional conflict analysis."""
    pattern_vs_similarity: str  # "aligned", "conflicting", "neutral"
    fraud_vs_counter_evidence: str
    deterministic_vs_llm: str
    overall_conflict_score: float  # 0.0 = no conflict, 1.0 = high conflict
    resolution_strategy: str  # "trust_pattern", "trust_similarity", "flag_for_review", "weighted_average"

    def to_dict(self) -> dict[str, Any]:
        return {
            "pattern_vs_similarity": self.pattern_vs_similarity,
            "fraud_vs_counter_evidence": self.fraud_vs_counter_evidence,
            "deterministic_vs_llm": self.deterministic_vs_llm,
            "overall_conflict_score": self.overall_conflict_score,
            "resolution_strategy": self.resolution_strategy
        }
```

**Computation Logic**
```python
def compute_conflict_matrix(
    pattern_analysis: dict[str, Any],
    similarity_result: SimilarityResult,
    counter_evidence: list[CounterEvidence],
    llm_reasoning: dict[str, Any] | None
) -> ConflictMatrix:
    """Compute multi-dimensional conflict matrix."""

    # Dimension 1: Pattern vs Similarity
    pattern_severity = pattern_analysis.get("severity", "LOW")
    similarity_score = similarity_result.overall_score

    if (pattern_severity in ("HIGH", "CRITICAL") and similarity_score > 0.6) or \
       (pattern_severity == "LOW" and similarity_score < 0.3):
        pattern_vs_sim = "aligned"
    elif (pattern_severity in ("HIGH", "CRITICAL") and similarity_score < 0.3) or \
         (pattern_severity == "LOW" and similarity_score > 0.6):
        pattern_vs_sim = "conflicting"
    else:
        pattern_vs_sim = "neutral"

    # Dimension 2: Fraud vs Counter-Evidence
    fraud_signals = pattern_severity in ("HIGH", "CRITICAL") or similarity_score > 0.5
    counter_evidence_strength = sum(e.strength for e in counter_evidence)

    if fraud_signals and counter_evidence_strength > 0.5:
        fraud_vs_counter = "conflicting"
    elif not fraud_signals and counter_evidence_strength > 0.5:
        fraud_vs_counter = "counter_evidence_dominant"
    elif fraud_signals and counter_evidence_strength <= 0.5:
        fraud_vs_counter = "fraud_dominant"
    else:
        fraud_vs_counter = "neutral"

    # Dimension 3: Deterministic vs LLM
    det_vs_llm = "neutral"
    if llm_reasoning:
        llm_risk = llm_reasoning.get("risk_assessment", "MEDIUM")
        det_risk = pattern_severity

        if (llm_risk == "HIGH" and det_risk == "LOW") or \
           (llm_risk == "LOW" and det_risk == "HIGH"):
            det_vs_llm = "conflicting"
        else:
            det_vs_llm = "aligned"

    # Compute overall conflict score
    conflict_count = sum([
        pattern_vs_sim == "conflicting",
        fraud_vs_counter == "conflicting",
        det_vs_llm == "conflicting"
    ])
    overall_conflict = conflict_count / 3.0

    # Determine resolution strategy
    if overall_conflict > 0.6:
        resolution = "flag_for_review"
    elif fraud_vs_counter == "counter_evidence_dominant":
        resolution = "trust_counter_evidence"
    elif pattern_vs_sim == "conflicting":
        resolution = "weighted_average"
    else:
        resolution = "trust_deterministic"

    return ConflictMatrix(
        pattern_vs_similarity=pattern_vs_sim,
        fraud_vs_counter_evidence=fraud_vs_counter,
        deterministic_vs_llm=det_vs_llm,
        overall_conflict_score=overall_conflict,
        resolution_strategy=resolution
    )
```

**Database Storage**
```sql
-- Add to ops_agent_runs table
ALTER TABLE fraud_gov.ops_agent_runs
ADD COLUMN IF NOT EXISTS conflict_matrix JSONB;

-- Example stored value
{
  "pattern_vs_similarity": "conflicting",
  "fraud_vs_counter_evidence": "conflicting",
  "deterministic_vs_llm": "aligned",
  "overall_conflict_score": 0.67,
  "resolution_strategy": "flag_for_review"
}
```

---

### Improvement 4: Enhanced Narrative Generation

#### Overview
Generate structured, context-rich narratives with domain-specific language and case-specific formatting.

#### Technical Details

**Enhanced Prompt Template**
```python
# app/llm/prompts/investigation_v2.py
INVESTIGATION_V2 = PromptTemplate(
    name="investigation",
    version="2",
    system_prompt="""You are a senior fraud analyst with 15+ years of experience in card-not-present fraud detection.

Your role is to provide clear, actionable analysis that helps human analysts make informed decisions quickly.

**Guidelines:**
1. Lead with the most critical evidence first
2. Use specific domain terminology (e.g., "card testing," "cross-merchant bust-out")
3. Quantify risk with precise metrics (percentages, time windows, transaction counts)
4. Call out conflicts between evidence types explicitly
5. Distinguish between observed facts vs inferred patterns
6. Reference specific transaction IDs for cross-checking
7. Avoid hedging language - state confidence clearly
8. Flag any data quality issues or missing context

**Output Format (JSON):**
```json
{
    "narrative_summary": "2-3 sentence executive summary",
    "risk_assessment": "CRITICAL|HIGH|MEDIUM|LOW",
    "confidence": 0.0-1.0,
    "key_findings": [
        {
            "category": "pattern|similarity|counter_evidence|conflict",
            "finding": "Specific observation",
            "evidence_strength": "strong|moderate|weak",
            "transaction_ids": ["tx_id1", "tx_id2"]
        }
    ],
    "conflict_summary": "If conflicts exist, explain resolution",
    "recommended_actions": ["action1", "action2"],
    "data_quality_notes": "Any concerns about data completeness"
}
```""",

    user_template="""**Transaction Under Investigation:**
- Transaction ID: {transaction_id}
- Amount: ${amount} {currency}
- Timestamp: {timestamp}
- Card: {card_last4}
- Merchant: {merchant_id} (MCC: {merchant_category})
- Decision: {decision}
- 3DS Authenticated: {three_ds_authenticated}

**Pattern Analysis:**
{pattern_analysis}

**Similarity Analysis:**
{similarity_analysis}

**Counter-Evidence:**
{counter_evidence}

**Conflict Matrix:**
{conflict_matrix}

**Context:**
- Card Age: {card_age_days} days
- Transaction History (90d): {transaction_count_90d} txs, {approval_rate_90d:.1%} approval
- Velocity Last 24h: {velocity_24h} txs
- Device Fingerprint: {device_fingerprint}

Provide your analysis in JSON format."""
)
```

**Example Output**
```json
{
    "narrative_summary": "High-risk transaction showing cross-merchant card testing pattern with 7 similar declines in past 24 hours. Strong counter-evidence from successful 3DS on 2 prior similar transactions requires review.",
    "risk_assessment": "HIGH",
    "confidence": 0.85,
    "key_findings": [
        {
            "category": "pattern",
            "finding": "Velocity anomaly: 7 declined transactions across 5 merchants in 24h, amounts cluster $10-$50",
            "evidence_strength": "strong",
            "transaction_ids": ["tx_001", "tx_002", "tx_003"]
        },
        {
            "category": "similarity",
            "finding": "Vector similarity 0.78 to 3 known fraud cases (merchant cluster electronics resellers)",
            "evidence_strength": "strong",
            "transaction_ids": ["fraud_tx_001", "fraud_tx_002", "fraud_tx_003"]
        },
        {
            "category": "counter_evidence",
            "finding": "2 prior similar transactions had successful 3DS authentication with this device",
            "evidence_strength": "moderate",
            "transaction_ids": ["tx_004", "tx_005"]
        },
        {
            "category": "conflict",
            "finding": "Pattern similarity conflicts with 3DS success - recommend human review of device reputation",
            "evidence_strength": "moderate",
            "transaction_ids": []
        }
    ],
    "conflict_summary": "Strong fraud patterns conflict with moderate counter-evidence from successful 3DS. Resolution strategy: flag_for_review with priority score 0.75",
    "recommended_actions": [
        "Review device reputation history (prior 90 days)",
        "Verify 3DS authentication logs for authenticity",
        "Consider placing temporary velocity hold on card"
    ],
    "data_quality_notes": "Device fingerprint present. Merchant category code verified. No anomalies detected."
}
```

**Configuration**
```python
# app/core/config.py
class NarrativeConfig(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="NARRATIVE_")

    template_version: str = "v2"  # v1 = basic, v2 = enhanced
    max_key_findings: int = 10
    include_conflict_summary: bool = True
    include_recommended_actions: bool = True
    include_data_quality_notes: bool = True
    min_confidence_threshold: float = 0.6
```

---

### Improvement 5: Explanation Builder

#### Overview
Generate human-readable, section-formatted explanations with structured headers and bullet points.

#### Technical Details

**Explanation Schema**
```python
@dataclass(frozen=True)
class ExplanationSection:
    """Single section of explanation."""
    title: str
    content: str  # Markdown-formatted
    priority: int  # 1 = highest

@dataclass(frozen=True)
class Explanation:
    """Full explanation with sections."""
    investigation_id: str
    transaction_id: str
    sections: list[ExplanationSection]
    metadata: dict[str, Any]
    generated_at: datetime

    def to_markdown(self) -> str:
        """Render explanation as markdown document."""
        lines = [
            f"# Investigation Report",
            f"**Transaction ID:** {self.transaction_id}",
            f"**Generated:** {self.generated_at.isoformat()}",
            ""
        ]

        for section in sorted(self.sections, key=lambda s: s.priority):
            lines.append(f"## {section.title}")
            lines.append(section.content)
            lines.append("")

        return "\n".join(lines)
```

**Section Builders**
```python
# app/agents/explanation_builder.py

class ExplanationBuilder:
    """Generate human-readable explanations."""

    def build(
        self,
        context: dict,
        pattern_analysis: dict,
        similarity_result: SimilarityResult,
        conflict_matrix: ConflictMatrix,
        llm_reasoning: dict | None
    ) -> Explanation:
        """Build full explanation with sections."""

        sections = []

        # Section 1: Executive Summary (Priority 1)
        sections.append(self._executive_summary(context, llm_reasoning))

        # Section 2: Pattern Analysis (Priority 2)
        sections.append(self._pattern_analysis(pattern_analysis))

        # Section 3: Similarity Analysis (Priority 3)
        sections.append(self._similarity_analysis(similarity_result))

        # Section 4: Counter-Evidence (Priority 4)
        sections.append(self._counter_evidence(similarity_result))

        # Section 5: Conflict Resolution (Priority 5)
        sections.append(self._conflict_resolution(conflict_matrix))

        # Section 6: Recommended Actions (Priority 6)
        sections.append(self._recommended_actions(context, conflict_matrix))

        return Explanation(
            investigation_id=context.get("investigation_id"),
            transaction_id=context.get("transaction_id"),
            sections=sections,
            metadata={
                "model_mode": "hybrid" if llm_reasoning else "deterministic",
                "llm_confidence": llm_reasoning.get("confidence") if llm_reasoning else None
            },
            generated_at=datetime.now(UTC)
        )

    def _executive_summary(self, context: dict, llm_reasoning: dict | None) -> ExplanationSection:
        """Build executive summary section."""
        if llm_reasoning:
            summary = llm_reasoning.get("narrative_summary", "")
        else:
            summary = self._deterministic_summary(context)

        return ExplanationSection(
            title="Executive Summary",
            content=f"{summary}\n\n**Risk Level:** {context.get('severity', 'UNKNOWN')}",
            priority=1
        )

    def _pattern_analysis(self, pattern_analysis: dict) -> ExplanationSection:
        """Build pattern analysis section."""
        patterns = pattern_analysis.get("patterns", [])

        lines = ["### Detected Patterns\n"]

        for pattern in sorted(patterns, key=lambda p: p.get("score", 0), reverse=True):
            score = pattern.get("score", 0)
            name = pattern.get("pattern_name", "unknown")
            description = pattern.get("description", "")

            lines.append(f"**{name}** (Score: {score:.2f})")
            lines.append(f"- {description}")
            lines.append("")

        return ExplanationSection(
            title="Pattern Analysis",
            content="\n".join(lines),
            priority=2
        )

    def _similarity_analysis(self, similarity_result: SimilarityResult) -> ExplanationSection:
        """Build similarity analysis section."""
        lines = [
            f"### Similarity Score: {similarity_result.overall_score:.2f}\n",
            f"Found **{len(similarity_result.matches)}** similar transactions.\n"
        ]

        for match in similarity_result.matches[:5]:
            lines.append(f"- Transaction `{match.match_id}`")
            lines.append(f"  - Similarity: {match.similarity_score:.2f}")
            lines.append(f"  - Type: {match.match_type}")

            if match.counter_evidence:
                lines.append(f"  - Counter-Evidence:")
                for ce in match.counter_evidence:
                    lines.append(f"    - {ce.get('description', '')}")

        return ExplanationSection(
            title="Similarity Analysis",
            content="\n".join(lines),
            priority=3
        )

    def _counter_evidence(self, similarity_result: SimilarityResult) -> ExplanationSection:
        """Build counter-evidence section."""
        all_counter_evidence = []

        for match in similarity_result.matches:
            if match.counter_evidence:
                all_counter_evidence.extend(match.counter_evidence)

        if not all_counter_evidence:
            return ExplanationSection(
                title="Counter-Evidence",
                content="No counter-evidence detected.",
                priority=4
            )

        lines = ["### Evidence Reducing Fraud Risk\n"]

        for ce in all_counter_evidence:
            lines.append(f"- **{ce.get('type', 'unknown')}** (Strength: {ce.get('strength', 0):.2f})")
            lines.append(f"  - {ce.get('description', '')}")
            lines.append("")

        return ExplanationSection(
            title="Counter-Evidence",
            content="\n".join(lines),
            priority=4
        )

    def _conflict_resolution(self, conflict_matrix: ConflictMatrix) -> ExplanationSection:
        """Build conflict resolution section."""
        if conflict_matrix.overall_conflict_score < 0.3:
            return ExplanationSection(
                title="Conflict Resolution",
                content="No significant conflicts detected between evidence types.",
                priority=5
            )

        lines = [
            f"### Conflict Score: {conflict_matrix.overall_conflict_score:.2f}\n",
            f"**Resolution Strategy:** {conflict_matrix.resolution_strategy}\n",
            "**Conflicts Detected:**\n"
        ]

        if conflict_matrix.pattern_vs_similarity == "conflicting":
            lines.append("- Pattern analysis conflicts with similarity results")

        if conflict_matrix.fraud_vs_counter_evidence == "conflicting":
            lines.append("- Fraud signals conflict with counter-evidence")

        if conflict_matrix.deterministic_vs_llm == "conflicting":
            lines.append("- Deterministic analysis conflicts with LLM assessment")

        return ExplanationSection(
            title="Conflict Resolution",
            content="\n".join(lines),
            priority=5
        )

    def _recommended_actions(self, context: dict, conflict_matrix: ConflictMatrix) -> ExplanationSection:
        """Build recommended actions section."""
        lines = ["### Recommended Actions\n"]

        if conflict_matrix.resolution_strategy == "flag_for_review":
            lines.append("1. **Flag for human review** due to conflicting evidence")
            lines.append("2. Prioritize review based on conflict score")
        elif conflict_matrix.resolution_strategy == "trust_counter_evidence":
            lines.append("1. Consider downgrading risk level")
            lines.append("2. Monitor for additional evidence")
        else:
            lines.append("1. Proceed with standard fraud review process")

        return ExplanationSection(
            title="Recommended Actions",
            content="\n".join(lines),
            priority=6
        )
```

**API Response**
```json
{
    "explanation": {
        "investigation_id": "...",
        "transaction_id": "...",
        "sections": [
            {
                "title": "Executive Summary",
                "content": "High-risk transaction...",
                "priority": 1
            }
        ],
        "markdown": "# Investigation Report\n..."
    }
}
```

---

### Improvement 6: Freshness Weighting with Exponential Decay

#### Overview
Apply exponential time-decay function to weight evidence relevance based on transaction age.

#### Technical Details

**Exponential Decay Function**
```python
# app/agents/freshness.py

from datetime import UTC, datetime, timedelta
import math

def exponential_decay_weight(
    transaction_timestamp: datetime | None,
    half_life_hours: float = 24.0,
    max_weight: float = 1.0,
    min_weight: float = 0.1
) -> float:
    """Compute exponential decay weight.

    Args:
        transaction_timestamp: When the transaction occurred
        half_life_hours: Time for weight to halve (default 24h)
        max_weight: Maximum weight (for very recent transactions)
        min_weight: Minimum weight (floor for old transactions)

    Returns:
        Weight between min_weight and max_weight
    """
    if transaction_timestamp is None:
        return 0.5

    now = datetime.now(UTC)
    age_hours = (now - transaction_timestamp).total_seconds() / 3600.0

    if age_hours < 0:
        # Future transaction (clock skew)
        return max_weight

    # Exponential decay: w = max * e^(-lambda * t)
    # where lambda = ln(2) / half_life
    decay_constant = math.log(2.0) / half_life_hours
    weight = max_weight * math.exp(-decay_constant * age_hours)

    return max(weight, min_weight)
```

**Configuration by Evidence Type**
```python
FRESHNESS_CONFIG = {
    "pattern_velocity": {
        "half_life_hours": 6.0,   # Velocity patterns decay fast (6h)
        "max_weight": 1.0,
        "min_weight": 0.2
    },
    "similarity_vector": {
        "half_life_hours": 72.0,  # Similarity decays slow (3 days)
        "max_weight": 1.0,
        "min_weight": 0.3
    },
    "counter_evidence_3ds": {
        "half_life_hours": 168.0,  # 3DS evidence lasts long (7 days)
        "max_weight": 1.0,
        "min_weight": 0.5
    },
    "similarity_attribute": {
        "half_life_hours": 48.0,  # Attribute matching (2 days)
        "max_weight": 1.0,
        "min_weight": 0.2
    }
}

def compute_freshness_weight(
    evidence_type: str,
    transaction_timestamp: datetime | None
) -> float:
    """Compute freshness weight for evidence type."""
    config = FRESHNESS_CONFIG.get(evidence_type, FRESHNESS_CONFIG["similarity_attribute"])

    return exponential_decay_weight(
        transaction_timestamp=transaction_timestamp,
        half_life_hours=config["half_life_hours"],
        max_weight=config["max_weight"],
        min_weight=config["min_weight"]
    )
```

**Integration with Similarity Scoring**
```python
def evaluate_similarity(
    transaction: Any,
    similar_transactions: list[dict[str, Any]],
) -> SimilarityResult:
    """Evaluate similarity with freshness weighting."""
    matches = []

    for sim_tx in similar_transactions:
        base_score = compute_attribute_similarity(transaction, sim_tx)

        # Apply freshness weighting
        freshness = compute_freshness_weight(
            evidence_type="similarity_attribute",
            transaction_timestamp=sim_tx.get("transaction_timestamp")
        )

        weighted_score = base_score * freshness

        if weighted_score > 0.1:  # Filter very low scores
            matches.append(
                SimilarityMatch(
                    match_id=sim_tx["transaction_id"],
                    match_type="attribute",
                    similarity_score=weighted_score,
                    details={**sim_tx, "freshness_weight": freshness}
                )
            )

    matches.sort(key=lambda m: m.similarity_score, reverse=True)
    top_matches = matches[:5]

    overall = sum(m.similarity_score for m in top_matches) / len(top_matches) if top_matches else 0.0

    return SimilarityResult(matches=top_matches, overall_score=overall)
```

**Test Cases**
```python
# tests/unit/test_freshness.py
import pytest
from datetime import timedelta

def test_freshness_decay():
    now = datetime.now(UTC)

    # Very recent (< 1 hour) = max weight
    assert exponential_decay_weight(now - timedelta(minutes=30)) == pytest.approx(1.0)

    # At half-life = 0.5 weight
    assert exponential_decay_weight(now - timedelta(hours=24), half_life_hours=24.0) == pytest.approx(0.5)

    # 2 half-lives = 0.25 weight
    assert exponential_decay_weight(now - timedelta(hours=48), half_life_hours=24.0) == pytest.approx(0.25)

    # Very old = min weight floor
    assert exponential_decay_weight(now - timedelta(days=30)) == pytest.approx(0.1)

def test_evidence_type_configs():
    now = datetime.now(UTC)

    # Velocity decays faster than vector similarity
    velocity_weight = compute_freshness_weight("pattern_velocity", now - timedelta(hours=12))
    vector_weight = compute_freshness_weight("similarity_vector", now - timedelta(hours=12))

    assert velocity_weight < vector_weight
```

---

### Improvement 7: Structured Evidence Envelope

#### Overview
Standardize evidence storage format for queryability and consistency.

#### Technical Details

**Evidence Envelope Schema**
```python
@dataclass(frozen=True)
class EvidenceEnvelope:
    """Standardized evidence storage format."""

    # Metadata
    evidence_id: str
    evidence_kind: str  # "pattern", "similarity", "counter_evidence", "conflict", "llm_reasoning"
    investigation_id: str

    # Evidence Content
    category: str  # Sub-type (e.g., "velocity", "3ds_success")
    strength: float  # 0.0-1.0
    description: str

    # Supporting Data
    supporting_data: dict[str, Any]  # Structured, queryable fields

    # Temporal
    timestamp: datetime
    freshness_weight: float

    # Links
    related_transaction_ids: list[str]  # IDs of related transactions
    evidence_references: dict[str, Any]  # Links to other evidence

    def to_jsonb(self) -> dict:
        """Convert to JSONB-compatible dict for storage."""
        return {
            "evidence_id": self.evidence_id,
            "evidence_kind": self.evidence_kind,
            "category": self.category,
            "strength": self.strength,
            "description": self.description,
            "supporting_data": self.supporting_data,
            "timestamp": self.timestamp.isoformat(),
            "freshness_weight": self.freshness_weight,
            "related_transaction_ids": self.related_transaction_ids,
            "evidence_references": self.evidence_references
        }
```

**Database Schema**
```sql
-- Migration: 007_structured_evidence_envelope.sql
ALTER TABLE fraud_gov.ops_agent_evidence
ADD COLUMN IF NOT EXISTS category VARCHAR(100),
ADD COLUMN IF NOT EXISTS strength FLOAT,
ADD COLUMN IF NOT EXISTS timestamp TIMESTAMP WITH TIME ZONE,
ADD COLUMN IF NOT EXISTS freshness_weight FLOAT,
ADD COLUMN IF NOT EXISTS related_transaction_ids TEXT[],
ADD COLUMN IF NOT EXISTS evidence_references JSONB;

-- Create indexes for queryability
CREATE INDEX IF NOT EXISTS idx_evidence_kind_category
ON fraud_gov.ops_agent_evidence(evidence_kind, category);

CREATE INDEX IF NOT EXISTS idx_evidence_strength
ON fraud_gov.ops_agent_evidence(strength DESC)
WHERE strength > 0.5;

CREATE INDEX IF NOT EXISTS idx_evidence_timestamp
ON fraud_gov.ops_agent_evidence(timestamp DESC);

-- GIN index for related_transaction_ids array
CREATE INDEX IF NOT EXISTS idx_evidence_related_txns
ON fraud_gov.ops_agent_evidence USING GIN (related_transaction_ids);
```

**Evidence Builders**
```python
# app/agents/evidence_builder.py

class EvidenceBuilder:
    """Build structured evidence envelopes."""

    def build_pattern_evidence(
        self,
        investigation_id: str,
        pattern_name: str,
        score: float,
        description: str,
        supporting_data: dict
    ) -> EvidenceEnvelope:
        """Build pattern evidence envelope."""
        return EvidenceEnvelope(
            evidence_id=str(uuid7()),
            evidence_kind="pattern",
            investigation_id=investigation_id,
            category=pattern_name,
            strength=score,
            description=description,
            supporting_data=supporting_data,
            timestamp=datetime.now(UTC),
            freshness_weight=compute_freshness_weight("pattern_velocity", datetime.now(UTC)),
            related_transaction_ids=supporting_data.get("related_transaction_ids", []),
            evidence_references={}
        )

    def build_similarity_evidence(
        self,
        investigation_id: str,
        match: SimilarityMatch
    ) -> EvidenceEnvelope:
        """Build similarity evidence envelope."""
        return EvidenceEnvelope(
            evidence_id=str(uuid7()),
            evidence_kind="similarity",
            investigation_id=investigation_id,
            category=match.match_type,
            strength=match.similarity_score,
            description=f"Similar transaction ({match.match_type}): {match.match_id}",
            supporting_data=match.details,
            timestamp=datetime.now(UTC),
            freshness_weight=compute_freshness_weight(
                f"similarity_{match.match_type}",
                match.details.get("transaction_timestamp")
            ),
            related_transaction_ids=[match.match_id],
            evidence_references={
                "counter_evidence": match.counter_evidence or []
            }
        )

    def build_counter_evidence(
        self,
        investigation_id: str,
        counter_evidence: CounterEvidence
    ) -> EvidenceEnvelope:
        """Build counter-evidence envelope."""
        return EvidenceEnvelope(
            evidence_id=str(uuid7()),
            evidence_kind="counter_evidence",
            investigation_id=investigation_id,
            category=counter_evidence.evidence_type,
            strength=counter_evidence.strength,
            description=counter_evidence.description,
            supporting_data=counter_evidence.supporting_data,
            timestamp=datetime.now(UTC),
            freshness_weight=compute_freshness_weight(
                f"counter_evidence_{counter_evidence.evidence_type}",
                datetime.now(UTC)
            ),
            related_transaction_ids=counter_evidence.supporting_data.get("transaction_ids", []),
            evidence_references={}
        )

    def build_conflict_evidence(
        self,
        investigation_id: str,
        conflict_matrix: ConflictMatrix
    ) -> EvidenceEnvelope:
        """Build conflict matrix evidence envelope."""
        return EvidenceEnvelope(
            evidence_id=str(uuid7()),
            evidence_kind="conflict",
            investigation_id=investigation_id,
            category="resolution",
            strength=conflict_matrix.overall_conflict_score,
            description=f"Conflict analysis: {conflict_matrix.resolution_strategy}",
            supporting_data=conflict_matrix.to_dict(),
            timestamp=datetime.now(UTC),
            freshness_weight=1.0,  # Conflicts don't decay
            related_transaction_ids=[],
            evidence_references={}
        )
```

**Persistence**
```python
# app/persistence/evidence_repository.py (update)

class EvidenceRepository:
    async def insert_structured_evidence(
        self,
        insight_id: str,
        envelope: EvidenceEnvelope
    ) -> dict:
        """Insert structured evidence envelope."""
        query = text("""
            INSERT INTO fraud_gov.ops_agent_evidence
                (evidence_id, insight_id, evidence_kind, category, strength,
                 description, supporting_data, timestamp, freshness_weight,
                 related_transaction_ids, evidence_references, created_at)
            VALUES
                (:evidence_id, :insight_id, :evidence_kind, :category, :strength,
                 :description, :supporting_data, :timestamp, :freshness_weight,
                 :related_transaction_ids, :evidence_references, NOW())
            RETURNING evidence_id, insight_id, evidence_kind, category, strength
        """)

        result = await self.session.execute(query, {
            "evidence_id": envelope.evidence_id,
            "insight_id": insight_id,
            "evidence_kind": envelope.evidence_kind,
            "category": envelope.category,
            "strength": envelope.strength,
            "description": envelope.description,
            "supporting_data": json.dumps(envelope.supporting_data),
            "timestamp": envelope.timestamp,
            "freshness_weight": envelope.freshness_weight,
            "related_transaction_ids": envelope.related_transaction_ids,
            "evidence_references": json.dumps(envelope.evidence_references)
        })

        return dict(result.fetchone())
```

---

## 3. Implementation Roadmap

### Phase 1: Database Foundation (Weeks 1-2)

**Goal**: Enable vector similarity search and structured evidence storage.

#### Tasks

1. **pgvector Setup**
    - [x] Install pgvector extension on PostgreSQL (requires pgvector-enabled Postgres image)
    - [x] Create ops-agent-owned embeddings table (`ops_agent_transaction_embeddings`)
    - [x] Create ivfflat index with cosine similarity on embeddings table
    - [x] Test vector search performance

2. **Migrations**
    - [x] `006_add_transaction_embeddings_pgvector.sql`
    - [ ] `007_structured_evidence_envelope.sql`
    - [ ] Backfill embeddings for existing transactions (batch job)
    - [x] Verify index performance with EXPLAIN ANALYZE

3. **Embedding Generation**
    - [x] Support Ollama-compatible embeddings endpoint (mxbai-embed-large)
    - [ ] Create batch embedding script for backfill
    - [ ] Add embedding generation to transaction ingestion flow (future)

**Deliverables**:
- [x] Migration 006 applied (pgvector + embeddings table)
- [ ] Migration 007 applied (structured evidence envelope)
- [ ] Embeddings backfilled for 90-day window
- [x] Vector search query returning results < 100ms

**Testing**:
```sql
-- Verify vector search
EXPLAIN ANALYZE
SELECT id, 1 - (embedding <=> '[0.1, 0.2, ...]'::vector) as similarity
FROM fraud_gov.ops_agent_transaction_embeddings
ORDER BY embedding <=> '[0.1, 0.2, ...]'::vector
LIMIT 20;

-- Expected: Index scan on idx_transactions_embedding
```

---

### Phase 2: Core Engine Enhancements (Weeks 3-4)

**Goal**: Implement vector similarity, counter-evidence, and conflict matrix.

#### Tasks

1. **Similarity Engine**
    - [x] Update `similarity_engine_core.py` with vector-aware evaluation
    - [x] Update `similarity_engine.py` with pgvector queries
    - [x] Add attribute-based search fallback
    - [ ] Implement counter-evidence detection

2. **Conflict Matrix**
   - [ ] Create `conflict_matrix.py` module
   - [ ] Implement 3-dimensional conflict computation
   - [ ] Add resolution strategies
   - [ ] Update schema to store conflict_matrix JSONB

3. **Freshness Weighting**
   - [ ] Create `freshness.py` module (ENHANCED: existing bucket-based needs exponential decay upgrade)
   - [ ] Implement exponential decay function
   - [ ] Add evidence-type configs
   - [x] Integrate into similarity scoring

**Deliverables**:
- [x] Similarity engine returning vector + attribute matches
- [ ] Counter-evidence detected and stored
- [ ] Conflict matrix computed and persisted

**Testing**:
```python
# tests/integration/test_similarity_integration.py
@pytest.mark.asyncio
async def test_vector_search_counter_evidence():
    engine = SimilarityEngine(session)

    result = await engine.analyze(context)

    assert len(result["similarity_result"].matches) > 0
    assert any(m.counter_evidence for m in result["similarity_result"].matches)
    assert result["similarity_result"].conflict_matrix is not None
```

---

### Phase 3: LLM Enhancements (Weeks 5-6)

**Goal**: Implement enhanced narrative generation and explanation builder.

#### Tasks

1. **Prompt Template v2**
   - [ ] Create `investigation_v2.py` template
   - [ ] Add conflict summary section
   - [ ] Add counter-evidence section
   - [ ] Add recommended actions section

2. **Explanation Builder**
   - [ ] Create `explanation_builder.py`
   - [ ] Implement 6 section builders
   - [ ] Add markdown rendering
   - [ ] Add section prioritization

3. **Evidence Envelope**
   - [ ] Create `evidence_builder.py`
   - [ ] Implement envelope builders for all evidence types
   - [ ] Update `evidence_repository.py` to persist structured data
   - [ ] Add JSONB indexes for queryability

**Deliverables**:
- Enhanced narratives with 5-6 sections
- Explanation builder generating markdown reports
- Structured evidence stored and queryable

**Testing**:
```python
# tests/unit/test_explanation_builder.py
def test_explanation_sections():
    builder = ExplanationBuilder()

    explanation = builder.build(context, patterns, similarity, conflicts, llm)

    assert len(explanation.sections) == 6
    markdown = explanation.to_markdown()
    assert "# Investigation Report" in markdown
    assert "## Executive Summary" in markdown
```

---

### Phase 4: Integration & Rollout (Weeks 7-8)

**Goal**: Integrate all improvements and enable via feature flags.

#### Tasks

1. **Pipeline Integration**
   - [ ] Update `pipeline.py` to call new engines
   - [ ] Add conflict matrix to run completion
   - [ ] Store explanation in insight
   - [ ] Update API responses

2. **Feature Flags**
    - [x] Add `VECTOR_ENABLED` flag
   - [ ] Add `NARRATIVE_VERSION` flag
   - [ ] Add `COUNTER_EVIDENCE_ENABLED` flag
   - [ ] Add `CONFLICT_MATRIX_ENABLED` flag

3. **API Changes**
   - [ ] Add `/investigations/{id}/explanation` endpoint
   - [ ] Update GET `/transactions/{id}/insights` with conflict_matrix
   - [ ] Add explanation to investigation detail response

4. **Testing**
   - [ ] Update unit tests (expect +50 tests)
   - [ ] Update integration tests
   - [ ] Add E2E test for full pipeline
   - [ ] Performance testing (vector search latency)

5. **Documentation**
   - [ ] Update API docs
   - [ ] Update README with new features
   - [ ] Create migration guide
   - [ ] Update runbooks

**Deliverables**:
- All improvements integrated and feature-flagged
- E2E tests passing
- API documentation updated
- Rollout plan approved

**Testing**:
```python
# tests/e2e/test_improvements_e2e.py
@pytest.mark.e2e
async def test_full_pipeline_with_improvements():
    # Vector search enabled
    # Counter-evidence enabled
    # Narrative v2 enabled

    response = await client.post("/api/v1/investigations/run", json={...})

    assert response.status_code == 200
    data = response.json()

    assert "explanation" in data
    assert "conflict_matrix" in data
    assert len(data["evidence"]) > 0
    assert any(e["evidence_kind"] == "counter_evidence" for e in data["evidence"])
```

---

## 4. Technical Architecture

### 4.1 Component Diagram

```

                      API Layer                                
  POST /investigations/run                                     
  GET  /investigations/{id}/explanation                        

                      
                      

                   Services                                    
  investigation_service.py                                     
  - Orchestrates pipeline                                      
  - Checks feature flags                                       

                      
                      

                   Pipeline                                    
  pipeline.py                                                  
  1. Context Builder                                          
  2. Pattern Engine                                           
  3. Similarity Engine  NEW: Vector + Counter-Evidence      
  4. Conflict Matrix  NEW                                    
  5. Reasoning Engine  Enhanced prompts                     
  6. Explanation Builder  NEW                               
  7. Recommendation Engine                                    

                      
        
                                  
                                  
  
   Database       LLM        Vector     
  fraud_gov     Ollama      (pgvector)  
  
```

### 4.2 Data Flow

```
Transaction  Context Builder  Pattern Engine
                                   
                                   
                            Similarity Engine
                                   
                    
                                                
                                                
            Vector Search   Attr Search   Counter-Evidence
                                                
                    
                                   
                                   
                            Conflict Matrix
                                   
                                   
                            Reasoning Engine
                                   
                                   
                          Explanation Builder
                                   
                                   
                      Recommendation Engine
                                   
                                   
                            Persistence
```

### 4.3 Technology Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| **Vector Database** | PostgreSQL + pgvector | 16+ |
| **Embedding Model** | MixedBread mxbai-embed-large | 1.0 |
| **Inference** | Ollama | Latest |
| **LLM** | Ollama (Llama 3.2) / Claude Haiku | - |
| **Python** | CPython | 3.14+ |
| **Framework** | FastAPI | 0.115+ |
| **Database Driver** | asyncpg | 0.31+ |

---

## 5. Database Schema Changes

### Migration 006: Vector Similarity

```sql
-- db/migrations/006_add_similarity_vector.sql

-- Install pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- IMPORTANT: Avoid altering `fraud_gov.transactions` (owned by TM). Instead, store
-- embeddings in an ops-agent-owned table keyed by `transaction_id`.
--
-- NOTE: This introduces a new `ops_agent_*` table; if adopted, update:
-- - scripts/reset_tables.py and scripts/reset_data.py (truncate/drop only owned tables)
-- - db/ops_agent_schema.sql reference DDL
-- - AGENTS.md table allowlist

CREATE TABLE IF NOT EXISTS fraud_gov.ops_agent_transaction_embeddings (
    transaction_id UUID PRIMARY KEY REFERENCES fraud_gov.transactions(id) ON DELETE CASCADE,
    embedding vector(1024) NOT NULL,
    model_name TEXT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);

-- Create ivfflat index for fast approximate search
-- Lists parameter = sqrt(rows) for optimal performance
CREATE INDEX IF NOT EXISTS idx_ops_agent_tx_embeddings_embedding
ON fraud_gov.ops_agent_transaction_embeddings
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Grant permissions
GRANT SELECT, INSERT, UPDATE ON fraud_gov.ops_agent_transaction_embeddings TO fraud_gov_app_user;
```

### Migration 007: Structured Evidence

```sql
-- db/migrations/007_structured_evidence_envelope.sql

-- Add structured columns to evidence table
ALTER TABLE fraud_gov.ops_agent_evidence
ADD COLUMN IF NOT EXISTS category VARCHAR(100),
ADD COLUMN IF NOT EXISTS strength FLOAT CHECK (strength >= 0.0 AND strength <= 1.0),
ADD COLUMN IF NOT EXISTS timestamp TIMESTAMP WITH TIME ZONE,
ADD COLUMN IF NOT EXISTS freshness_weight FLOAT CHECK (freshness_weight >= 0.0 AND freshness_weight <= 1.0),
ADD COLUMN IF NOT EXISTS related_transaction_ids TEXT[],
ADD COLUMN IF NOT EXISTS evidence_references JSONB;

-- Create indexes for queryability
CREATE INDEX IF NOT EXISTS idx_evidence_kind_category
ON fraud_gov.ops_agent_evidence(evidence_kind, category);

CREATE INDEX IF NOT EXISTS idx_evidence_strength
ON fraud_gov.ops_agent_evidence(strength DESC)
WHERE strength > 0.5;

CREATE INDEX IF NOT EXISTS idx_evidence_timestamp
ON fraud_gov.ops_agent_evidence(timestamp DESC);

CREATE INDEX IF NOT EXISTS idx_evidence_related_txns
ON fraud_gov.ops_agent_evidence USING GIN (related_transaction_ids);

-- Add conflict matrix to runs table
ALTER TABLE fraud_gov.ops_agent_runs
ADD COLUMN IF NOT EXISTS conflict_matrix JSONB;

CREATE INDEX IF NOT EXISTS idx_runs_conflict_score
ON fraud_gov.ops_agent_runs((conflict_matrix->>'overall_conflict_score')::FLOAT DESC)
WHERE conflict_matrix IS NOT NULL;
```

### Backfill Embeddings Script

```python
# scripts/backfill_embeddings.py

import asyncio
import httpx
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy import text
import numpy as np

async def generate_embedding(text: str, api_base: str) -> np.ndarray:
    """Generate embedding via Ollama."""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{api_base}/embeddings",
            json={"model": "mxbai-embed-large", "prompt": text}
        )
        return np.array(response.json()["embedding"])

async def backfill_embeddings():
    """Backfill embeddings for 90-day window."""
    engine = create_async_engine("postgresql+asyncpg://...")

    async with engine.begin() as conn:
        # Fetch transactions without embeddings
        result = await conn.execute(text("""
            SELECT id, transaction_id, card_id, merchant_id,
                   transaction_amount, merchant_category_code
            FROM fraud_gov.transactions
            WHERE transaction_timestamp >= NOW() - INTERVAL '90 days'
              AND embedding IS NULL
            ORDER BY transaction_timestamp DESC
        """))

        transactions = result.fetchall()

        for tx in transactions:
            # Create text representation
            text = f"Transaction {tx.transaction_id} card {tx.card_id} merchant {tx.merchant_id} amount {tx.transaction_amount} mcc {tx.merchant_category_code}"

            # Generate embedding
            embedding = await generate_embedding(text, "http://localhost:11434/api")

            # Update transaction
            await conn.execute(text("""
                UPDATE fraud_gov.transactions
                SET embedding = :embedding
                WHERE id = :id
            """), {"id": tx.id, "embedding": str(embedding.tolist())})

            print(f"Backfilled: {tx.transaction_id}")

if __name__ == "__main__":
    asyncio.run(backfill_embeddings())
```

---

## 6. API Changes

### New Endpoints

#### GET /api/v1/investigations/{run_id}/explanation

Fetch human-readable explanation with formatted sections.

**Response:**
```json
{
    "investigation_id": "...",
    "transaction_id": "...",
    "sections": [
        {
            "title": "Executive Summary",
            "content": "High-risk transaction...",
            "priority": 1
        },
        {
            "title": "Pattern Analysis",
            "content": "### Detected Patterns\n...",
            "priority": 2
        }
    ],
    "markdown": "# Investigation Report\n...",
    "metadata": {
        "model_mode": "hybrid",
        "llm_confidence": 0.85,
        "generated_at": "2026-02-15T10:30:00Z"
    }
}
```

### Updated Endpoints

#### POST /api/v1/investigations/run

**Enhanced Response:**
```json
{
    "run_id": "...",
    "transaction_id": "...",
    "status": "COMPLETED",
    "insight": {
        "insight_id": "...",
        "severity": "HIGH",
        "summary": "...",
        "explanation": {  // NEW
            "sections": [...],
            "markdown": "..."
        },
        "conflict_matrix": {  // NEW
            "pattern_vs_similarity": "conflicting",
            "fraud_vs_counter_evidence": "conflicting",
            "deterministic_vs_llm": "aligned",
            "overall_conflict_score": 0.67,
            "resolution_strategy": "flag_for_review"
        },
        "evidence": [
            {
                "evidence_kind": "pattern",
                "category": "velocity",
                "strength": 0.85,
                "description": "...",
                "supporting_data": {...},
                "freshness_weight": 0.9
            },
            {
                "evidence_kind": "similarity",
                "category": "vector",
                "strength": 0.78,
                "description": "...",
                "supporting_data": {...},
                "freshness_weight": 0.85,
                "related_transaction_ids": ["...", "..."]
            },
            {
                "evidence_kind": "counter_evidence",  // NEW
                "category": "3ds_success",
                "strength": 0.8,
                "description": "2 similar transactions had successful 3DS",
                "supporting_data": {...},
                "freshness_weight": 1.0
            }
        ]
    },
    "recommendations": [...]
}
```

#### GET /api/v1/transactions/{transaction_id}/insights

**Enhanced Response:**
```json
{
    "transaction_id": "...",
    "insights": [
        {
            "insight_id": "...",
            "severity": "HIGH",
            "summary": "...",
            "model_mode": "hybrid",
            "explanation": {  // NEW
                "sections": [...]
            },
            "conflict_matrix": {...},  // NEW
            "evidence": [...],  // Structured envelopes
            "generated_at": "2026-02-15T10:30:00Z"
        }
    ]
}
```

### Schema Updates

```python
# app/schemas/v1/explanations.py

from pydantic import BaseModel
from typing import List

class ExplanationSection(BaseModel):
    title: str
    content: str
    priority: int

class ExplanationMetadata(BaseModel):
    model_mode: str
    llm_confidence: float | None = None
    generated_at: datetime

class ExplanationResponse(BaseModel):
    investigation_id: str
    transaction_id: str
    sections: List[ExplanationSection]
    markdown: str
    metadata: ExplanationMetadata

# app/schemas/v1/evidence.py

class EvidenceEnvelope(BaseModel):
    evidence_id: str
    evidence_kind: str
    category: str
    strength: float
    description: str
    supporting_data: dict[str, Any]
    timestamp: datetime
    freshness_weight: float
    related_transaction_ids: List[str] = []
    evidence_references: dict[str, Any] = {}

# app/schemas/v1/conflicts.py

class ConflictMatrixResponse(BaseModel):
    pattern_vs_similarity: str
    fraud_vs_counter_evidence: str
    deterministic_vs_llm: str
    overall_conflict_score: float
    resolution_strategy: str

# app/schemas/v1/insights.py (update)

class InsightDetail(BaseModel):
    insight_id: str
    severity: str
    summary: str
    model_mode: str
    explanation: ExplanationResponse | None = None  # NEW
    conflict_matrix: ConflictMatrixResponse | None = None  # NEW
    evidence: List[EvidenceEnvelope] = []  # Changed from unstructured dict
    generated_at: datetime
```

---

## 7. Code Structure

### New Files

```
app/
 agents/
    freshness.py                      # NEW: Exponential decay
    conflict_matrix.py                # NEW: Multi-dimensional conflicts
    explanation_builder.py            # NEW: Human-readable explanations
    evidence_builder.py               # NEW: Structured envelopes
    similarity_engine_core.py         # UPDATE: Add vector search
    similarity_engine.py              # UPDATE: Add pgvector queries
    reasoning_engine.py               # UPDATE: Enhanced prompts
 llm/
    prompts/
        investigation_v1.py           # KEEP: Basic version
        investigation_v2.py           # NEW: Enhanced version
 persistence/
    evidence_repository.py            # UPDATE: Structured storage
 schemas/
    v1/
        explanations.py               # NEW: Explanation schemas
        conflicts.py                  # NEW: Conflict schemas
        evidence.py                   # NEW: Evidence envelope schemas
 services/
     investigation_service.py           # UPDATE: Call new components
```

### Modified Files

```
app/
 agents/
    pipeline.py                        # UPDATE: Add conflict matrix, explanation
    similarity_engine_core.py          # UPDATE: Freshness weighting
 core/
    config.py                          # UPDATE: New feature flags
 api/
     routes/
         investigations.py               # UPDATE: /explanation endpoint
```

---

## 8. Testing Strategy

### Unit Tests (Target: +50 tests)

**New Test Files:**
```
tests/unit/
 test_freshness.py                      # 10 tests
 test_conflict_matrix.py                # 15 tests
 test_explanation_builder.py            # 10 tests
 test_evidence_builder.py                # 10 tests
 test_similarity_vector.py              # 15 tests
```

**Test Coverage Targets:**
- `freshness.py`: 100%
- `conflict_matrix.py`: 100%
- `explanation_builder.py`: 95%
- `evidence_builder.py`: 95%
- `similarity_engine.py`: 90% (excluding DB calls)

**Example Tests:**
```python
# tests/unit/test_freshness.py
import pytest
from datetime import timedelta, datetime

class TestExponentialDecay:
    def test_recent_transaction_max_weight(self):
        weight = exponential_decay_weight(datetime.now(UTC) - timedelta(minutes=30))
        assert weight == pytest.approx(1.0)

    def test_half_life_decay(self):
        weight = exponential_decay_weight(
            datetime.now(UTC) - timedelta(hours=24),
            half_life_hours=24.0
        )
        assert weight == pytest.approx(0.5)

    def test_min_weight_floor(self):
        weight = exponential_decay_weight(datetime.now(UTC) - timedelta(days=90))
        assert weight == pytest.approx(0.1)

# tests/unit/test_conflict_matrix.py
class TestConflictMatrix:
    def test_aligned_no_conflict(self):
        matrix = compute_conflict_matrix(
            pattern_analysis={"severity": "HIGH"},
            similarity_result=SimilarityResult(overall_score=0.8),
            counter_evidence=[],
            llm_reasoning={"risk_assessment": "HIGH"}
        )

        assert matrix.pattern_vs_similarity == "aligned"
        assert matrix.overall_conflict_score == 0.0
        assert matrix.resolution_strategy == "trust_deterministic"

    def test_conflicting_flag_review(self):
        matrix = compute_conflict_matrix(
            pattern_analysis={"severity": "HIGH"},
            similarity_result=SimilarityResult(overall_score=0.1),
            counter_evidence=[CounterEvidence(strength=0.9)],
            llm_reasoning={"risk_assessment": "LOW"}
        )

        assert matrix.overall_conflict_score > 0.6
        assert matrix.resolution_strategy == "flag_for_review"
```

### Integration Tests (Target: +10 tests)

**New Test Files:**
```
tests/integration/
 test_similarity_integration.py        # 5 tests
 test_evidence_envelope_integration.py # 5 tests
```

**Example Tests:**
```python
# tests/integration/test_similarity_integration.py
@pytest.mark.asyncio
async async def test_vector_search_with_counter_evidence(db_session):
    engine = SimilarityEngine(db_session)

    # Create test transactions with embeddings
    tx1 = create_transaction_with_embedding(...)
    tx2 = create_transaction_with_embedding(...)
    await db_session.commit()

    result = await engine.analyze({"transaction": tx1})

    assert len(result["similarity_result"].matches) > 0
    assert any(m.counter_evidence for m in result["similarity_result"].matches)
```

### E2E Tests (Target: +2 tests)

**New Test File:**
```
tests/e2e/
 test_improvements_e2e.py              # 2 tests
```

**Example Tests:**
```python
@pytest.mark.e2e
async async def test_full_pipeline_with_improvements(live_server):
    # Run investigation with all improvements enabled
    response = await live_server.post("/api/v1/investigations/run", json={
        "transaction_id": test_transaction_id,
        "mode": "deep"
    })

    assert response.status_code == 200
    data = response.json()

    # Verify new fields
    assert "explanation" in data["insight"]
    assert "conflict_matrix" in data["insight"]
    assert len(data["insight"]["evidence"]) > 0

    # Verify counter-evidence detection
    counter_evidence = [
        e for e in data["insight"]["evidence"]
        if e["evidence_kind"] == "counter_evidence"
    ]
    assert len(counter_evidence) >= 0  # May or may not exist

@pytest.mark.e2e
async async def test_explanation_endpoint(live_server):
    # Create investigation first
    run_response = await live_server.post("/api/v1/investigations/run", json={...})

    # Fetch explanation
    explanation_response = await live_server.get(
        f"/api/v1/investigations/{run_response.json()['run_id']}/explanation"
    )

    assert explanation_response.status_code == 200
    explanation = explanation_response.json()

    assert len(explanation["sections"]) == 6
    assert "# Investigation Report" in explanation["markdown"]
```

### Performance Tests

**Vector Search Latency:**
```python
@pytest.mark.asyncio
async async def test_vector_search_performance(db_session):
    engine = SimilarityEngine(db_session)

    start = time.time()
    result = await engine.analyze(context)
    latency = time.time() - start

    # Vector search should be < 100ms
    assert latency < 0.1
```

**Embedding Generation Throughput:**
```bash
# Test backfill performance
time uv run python scripts/backfill_embeddings.py

# Target: 100 embeddings/minute on local Ollama
```

---

## 9. Rollout Plan

### Feature Flag Configuration

Current repository baseline (2026-02-18):
- `VECTOR_ENABLED=true`
- `OPS_AGENT_ENABLE_LLM_REASONING=true`

Use the phased configuration below only for controlled rollout experiments, not as the default local/runtime baseline.

**Phase 1: Baseline Runtime (Weeks 1-2)**
```bash
# Keep core intelligence paths ON
VECTOR_ENABLED=true
OPS_AGENT_ENABLE_LLM_REASONING=true
NARRATIVE_VERSION=v1
OPS_AGENT_COUNTER_EVIDENCE_ENABLED=false
OPS_AGENT_CONFLICT_MATRIX_ENABLED=false
OPS_AGENT_EXPLANATION_BUILDER_ENABLED=false
```

**Phase 2: Internal Testing (Weeks 3-4)**
```bash
# Enable vector search for 10% of traffic
VECTOR_ENABLED=true
VECTOR_SEARCH_LIMIT=10  # Reduced limit

# Counter-evidence detection
OPS_AGENT_COUNTER_EVIDENCE_ENABLED=true

# Conflict matrix (read-only, no impact)
OPS_AGENT_CONFLICT_MATRIX_ENABLED=true
```

**Phase 3: Staged Rollout (Weeks 5-6)**
```bash
# 50% of traffic
VECTOR_ENABLED=true
VECTOR_SEARCH_LIMIT=20  # Full limit

# Enhanced narratives
NARRATIVE_VERSION=v2

# Explanation builder (admin users only)
OPS_AGENT_EXPLANATION_BUILDER_ENABLED=true
```

**Phase 4: Full Rollout (Weeks 7-8)**
```bash
# 100% of traffic
VECTOR_ENABLED=true
NARRATIVE_VERSION=v2
OPS_AGENT_COUNTER_EVIDENCE_ENABLED=true
OPS_AGENT_CONFLICT_MATRIX_ENABLED=true
OPS_AGENT_EXPLANATION_BUILDER_ENABLED=true
```

### Gradual Traffic Routing

```python
# app/core/config.py

class FeatureFlagsConfig(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="OPS_AGENT_")

    # Existing flags
    enable_llm_reasoning: bool = True
    enable_rule_draft_export: bool = False

    # New flags
    vector_enabled: bool = True
    narrative_version: str = "v1"
    counter_evidence_enabled: bool = False
    conflict_matrix_enabled: bool = False
    explanation_builder_enabled: bool = False

    # Traffic sampling
    vector_traffic_percentage: int = 100  # 0-100
    narrative_traffic_percentage: int = 100

    @field_validator('vector_traffic_percentage')
    @classmethod
    def validate_percentage(cls, v: int) -> int:
        if not 0 <= v <= 100:
            raise ValueError('Percentage must be 0-100')
        return v

# app/services/investigation_service.py

class InvestigationService:
    async def run_investigation(self, request: RunRequest, user: AuthenticatedUser):
        # Traffic sampling for vector search
        use_vector = self.settings.features.vector_enabled
        if self.settings.features.vector_enabled:
            if random.randint(0, 100) < self.settings.features.vector_traffic_percentage:
                use_vector = True

        # Run pipeline with feature flags
        result = await self.pipeline.run(
            context=context,
            use_vector=use_vector,
            use_counter_evidence=self.settings.features.counter_evidence_enabled,
            narrative_version=self.settings.features.narrative_version,
            # ...
        )
```

### Monitoring & Observability

**Metrics to Track:**
```python
# app/core/metrics.py

# New metrics
vector_search_latency = Histogram(
    "ops_agent_vector_search_latency_seconds",
    "Vector search latency",
    ["status"]
)

counter_evidence_detected = Counter(
    "ops_agent_counter_evidence_detected_total",
    "Counter-evidence detected",
    ["evidence_type"]
)

conflict_score_distribution = Histogram(
    "ops_agent_conflict_score",
    "Conflict matrix score distribution"
)

narrative_version_usage = Counter(
    "ops_agent_narrative_version_total",
    "Narrative template version usage",
    ["version"]
)

explanation_generation_latency = Histogram(
    "ops_agent_explanation_generation_latency_seconds",
    "Explanation generation latency"
)
```

**Alerts:**
- Vector search P95 > 200ms  Investigate index
- Embedding generation failure rate > 5%  Check Ollama
- Conflict score > 0.8 > 10% of traffic  Review patterns
- Narrative generation failure rate > 1%  Check prompts

### Rollback Plan

**Immediate Rollback Triggers:**
- Vector search failure rate > 10%
- Investigation latency P95 > 30s (baseline: ~5s)
- Counter-evidence false positive rate > 20%

**Rollback Steps:**
```bash
# 1. Disable features via Doppler
VECTOR_ENABLED=false  # emergency-only; restore to true after incident
NARRATIVE_VERSION=v1
OPS_AGENT_COUNTER_EVIDENCE_ENABLED=false

# 2. Restart services
kubectl rollout restart deployment/ops-analyst-agent

# 3. Verify rollback
curl https://api.example.com/api/v1/health

# 4. Monitor metrics for 15 minutes
# Check vector search traffic drops to zero
```

---

## 10. Risk Mitigation

### Risk 1: Vector Search Performance

**Risk**: pgvector queries slow down investigation pipeline.

**Mitigation**:
- Use ivfflat index (not HNSW) for lower memory overhead
- Set `lists = 100` for optimal performance
- Limit search to 90-day window
- Cache embeddings for frequently accessed transactions
- Fallback to attribute-based search if vector search times out

**Implementation**:
```python
# app/agents/similarity_engine.py

async def _vector_search_with_timeout(self, ...):
    try:
        return await asyncio.wait_for(
            self._vector_search(...),
            timeout=0.1  # 100ms timeout
        )
    except asyncio.TimeoutError:
        logger.warning("Vector search timed out, falling back to attribute search")
        return await self._attribute_search(...)
```

### Risk 2: Embedding Quality

**Risk**: mxbai-embed-large produces poor embeddings for financial data.

**Mitigation**:
- Test embedding quality before backfill (sample 100 transactions)
- Manual review of top 10 vector matches for sanity check
- If cosine similarity < 0.5 for all matches, disable vector search
- Fallback to attribute-based search

**Validation Script**:
```python
# scripts/validate_embeddings.py

async def validate_embedding_quality():
    """Test embedding quality on sample data."""
    sample_transactions = get_sample_transactions(100)

    for tx in sample_transactions:
        embedding = generate_embedding(tx)
        matches = vector_search(embedding)

        # Check if top matches are semantically similar
        top_match = matches[0]
        if top_match.similarity_score < 0.5:
            print(f"WARNING: Low similarity score for {tx.transaction_id}")
            print(f"  Top match: {top_match.transaction_id} (score: {top_match.similarity_score})")
```

### Risk 3: Counter-Evidence False Positives

**Risk**: Counter-evidence incorrectly reduces fraud risk.

**Mitigation**:
- Counter-evidence only reduces risk, never eliminates it
- Require 2+ counter-evidence instances for strength > 0.7
- Flag for human review if fraud_score > 0.7 AND counter_evidence > 0.5
- Monitor false negative rate after rollout

**Policy**:
```python
# app/agents/recommendation_engine_core.py

def apply_counter_evidence_discount(
    base_risk_score: float,
    counter_evidence: list[CounterEvidence]
) -> float:
    """Apply counter-evidence discount with safeguards."""

    if not counter_evidence:
        return base_risk_score

    # Only apply discount if multiple pieces of counter-evidence
    if len(counter_evidence) < 2:
        return base_risk_score

    total_strength = sum(ce.strength for ce in counter_evidence)
    discount_factor = min(total_strength * 0.3, 0.5)  # Max 50% discount

    discounted_score = base_risk_score * (1.0 - discount_factor)

    # Flag for review if high fraud + strong counter-evidence
    if base_risk_score > 0.7 and total_strength > 0.5:
        # Still flag for human review despite discount
        return max(discounted_score, 0.6)

    return discounted_score
```

### Risk 4: LLM Prompt Failure

**Risk**: Enhanced v2 prompts cause LLM errors or malformed JSON.

**Mitigation**:
- Keep v1 template as fallback
- Parse errors trigger automatic fallback to v1
- Monitor parse_error rate metric
- If parse_error rate > 5%, auto-switch to v1

**Implementation**:
```python
# app/llm/prompts/investigation_v2.py

async def render_investigation_prompt_v2(context):
    """Render v2 prompt with fallback to v1."""
    try:
        return await render_template_v2(context)
    except (KeyError, ValueError) as e:
        logger.warning(f"v2 prompt failed: {e}, falling back to v1")
        return await render_template_v1(context)
```

### Risk 5: Database Storage Bloat

**Risk**: Structured evidence envelopes increase storage requirements 2-3x.

**Mitigation**:
- Estimate storage: 100k investigations * 5 evidence * 2KB = 1GB (acceptable)
- Partition evidence table by created_at (monthly partitions)
- Archive old evidence (> 1 year) to cold storage
- Monitor table growth in weekly ops reviews

**Partitioning**:
```sql
-- db/migrations/008_partition_evidence.sql

CREATE TABLE fraud_gov.ops_agent_evidence_2026_02
PARTITION OF fraud_gov.ops_agent_evidence
FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');

CREATE TABLE fraud_gov.ops_agent_evidence_2026_03
PARTITION OF fraud_gov.ops_agent_evidence
FOR VALUES FROM ('2026-03-01') TO ('2026-04-01');
```

### Risk 6: Feature Flag Complexity

**Risk**: Too many flags make system difficult to reason about.

**Mitigation**:
- Group related flags (e.g., `IMPROVEMENTS_ENABLED` master flag)
- Document flag interactions in runbook
- Test all flag combinations in pre-prod
- Remove flags after 4 weeks of stable operation

**Master Flag**:
```python
# app/core/config.py

class FeatureFlagsConfig(BaseSettings):
    # Master flag
    improvements_enabled: bool = True

    # Individual flags (only read if master is true)
    vector_enabled: bool = True
    counter_evidence_enabled: bool = False
    narrative_version: str = "v1"

    @field_validator('vector_enabled')
    @classmethod
    def check_master_flag(cls, v: bool, info: ValidationInfo):
        if v and not info.data.get('improvements_enabled'):
            return False  # Force false if master flag is off
        return v
```

---

## 11. Success Metrics

### Quantitative Metrics

| Metric | Baseline | Target (8 weeks) | Measurement |
|--------|----------|-----------------|-------------|
| **Vector Search Latency** | N/A | < 100ms P95 | Histogram metric |
| **Investigation Latency** | ~5s | < 10s P95 | Investigation duration |
| **Fraud Detection Precision** | ~65% | ~80% (+15%) | Human review sample |
| **False Positive Rate** | ~35% | ~25% (-10%) | Analyst feedback |
| **Analyst Review Time** | ~10 min | ~6 min (-40%) | Time-to-close |
| **Counter-Evidence Detection** | 0% | 20% of cases | Evidence kind count |
| **Conflict Score Distribution** | N/A | Mean < 0.4 | Conflict matrix |
| **Narrative Token Count** | ~200 | ~800 (4x) | LLM usage |
| **Explanation Generation** | N/A | < 500ms | Explanation latency |
| **Embedding Backfill** | 0% | 100% of 90d | Transaction coverage |

### Qualitative Metrics

**Analyst Feedback Survey (after 4 weeks):**
- Narrative clarity: Target 4.5/5 stars
- Explanation usefulness: Target 4.0/5 stars
- Counter-evidence accuracy: Target < 10% false positives
- Conflict flagging: Target < 15% false positives

### Dashboard Queries

```sql
-- Vector search performance
SELECT
    percentile_cont(0.95) WITHIN GROUP (ORDER BY latency_ms) as p95_latency_ms
FROM ops_agent_audit_log
WHERE action = 'vector_search'
  AND created_at >= NOW() - INTERVAL '24 hours';

-- Counter-evidence detection rate
SELECT
    COUNT(*) FILTER (WHERE evidence_kind = 'counter_evidence') as counter_evidence_count,
    COUNT(*) as total_evidence,
    ROUND(100.0 * COUNT(*) FILTER (WHERE evidence_kind = 'counter_evidence') / COUNT(*), 2) as detection_rate_pct
FROM ops_agent_evidence
WHERE created_at >= NOW() - INTERVAL '7 days';

-- Conflict score distribution
SELECT
    (conflict_matrix->>'overall_conflict_score')::FLOAT as conflict_score,
    COUNT(*) as count
FROM ops_agent_runs
WHERE created_at >= NOW() - INTERVAL '7 days'
  AND conflict_matrix IS NOT NULL
GROUP BY (conflict_matrix->>'overall_conflict_score')::FLOAT
ORDER BY conflict_score;

-- Narrative version usage
SELECT
    model_mode,
    COUNT(*) as count
FROM ops_agent_insights
WHERE generated_at >= NOW() - INTERVAL '7 days'
GROUP BY model_mode;
```

---

## 12. Implementation Checklist

### Phase 1: Database (Weeks 1-2)

- [x] Install pgvector extension (requires pgvector-enabled Postgres image)
- [x] Run migration 006 (`006_add_transaction_embeddings_pgvector.sql`)
- [x] Run migration 007
- [ ] Create backfill embeddings script
- [ ] Backfill embeddings for 90-day window
- [x] Verify vector search performance (< 100ms)
- [x] Write integration tests for vector search

### Phase 2: Core Engines (Weeks 3-4)

- [x] Implement `freshness.py` module (exponential decay implemented)
- [x] Implement `conflict_matrix.py` module
- [x] Update `similarity_engine_core.py` with vector-aware evaluation
- [x] Update `similarity_engine.py` with pgvector queries
- [x] Add counter-evidence detection (core module updated)
- [x] Integrate freshness weighting (exponential decay in freshness.py)
- [ ] Update schema to store conflict_matrix
- [x] Write unit tests for all modules

### Phase 3: LLM Enhancements (Weeks 5-6)

- [x] Create `investigation_v2.py` prompt template
- [x] Implement `explanation_builder.py`
- [x] Implement `evidence_builder.py`
- [ ] Update `evidence_repository.py` for structured storage
- [ ] Add explanation endpoint to API
- [ ] Update investigation detail response
- [x] Write unit tests for explanation builder

### Phase 4: Integration (Weeks 7-8)

- [ ] Update `pipeline.py` to call new components
- [x] Add `VECTOR_ENABLED` feature flag to config
- [x] Add `NARRATIVE_VERSION` flag
- [x] Add `COUNTER_EVIDENCE_ENABLED` flag
- [x] Add `CONFLICT_MATRIX_ENABLED` flag
- [ ] Implement traffic sampling logic
- [ ] Add new metrics
- [ ] Update API documentation
- [ ] Write E2E tests for full pipeline
- [ ] Internal testing with 10% traffic
- [ ] Gradual rollout to 100%
- [ ] Monitor metrics and tune

### Documentation

- [ ] Update README with new features
- [ ] Create migration guide
- [ ] Update runbooks
- [ ] Create training materials for analysts
- [ ] Write ADR for vector similarity approach

---

## 13. References

### External Documentation

- **pgvector GitHub**: https://github.com/pgvector/pgvector
- **MixedBread Embeddings**: https://www.mixedbread.ai/blog/mxbai-embed-large
- **Ollama Embeddings**: https://ollama.com/blog/embedding-models
- **Cosine Similarity**: https://en.wikipedia.org/wiki/Cosine_similarity
- **Exponential Decay**: https://en.wikipedia.org/wiki/Exponential_decay

### Internal Documentation

- `docs/02-development/architecture.md` - System architecture
- `docs/02-development/domain-and-data-model.md` - Data model
- `docs/06-operations/observability.md` - Metrics and monitoring
- `docs/07-reference/0004-hybrid-deterministic-plus-llm-pipeline.md` - Pipeline design

### Code References

- `app/agents/similarity_engine.py` - Existing similarity engine
- `app/llm/prompts/investigation_v1.py` - Existing prompt template
- `app/agents/recommendation_engine.py` - Recommendation generation

---

## Appendix A: Configuration Examples

### Doppler Configuration (Local)

```yaml
# Vector Search
VECTOR_ENABLED=true
VECTOR_MODEL_NAME=mxbai-embed-large
VECTOR_INFERENCE_PROVIDER=ollama
VECTOR_API_BASE=http://localhost:11434/api
VECTOR_DIMENSION=1024
VECTOR_SEARCH_LIMIT=20
VECTOR_TIME_WINDOW_DAYS=90

# Narrative
NARRATIVE_VERSION=v2
NARRATIVE_MAX_KEY_FINDINGS=10
NARRATIVE_INCLUDE_CONFLICT_SUMMARY=true
NARRATIVE_INCLUDE_RECOMMENDED_ACTIONS=true

# Counter-Evidence
OPS_AGENT_COUNTER_EVIDENCE_ENABLED=true

# Conflict Matrix
OPS_AGENT_CONFLICT_MATRIX_ENABLED=true

# Explanation Builder
OPS_AGENT_EXPLANATION_BUILDER_ENABLED=true

# Traffic Sampling
OPS_AGENT_VECTOR_TRAFFIC_PERCENTAGE=100
OPS_AGENT_NARRATIVE_TRAFFIC_PERCENTAGE=100
```

### Docker Compose Services

```yaml
# card-fraud-platform/docker-compose.apps.yml (update)

services:
  ops-analyst-agent:
    environment:
      - VECTOR_ENABLED=${VECTOR_ENABLED:-true}
      - OPS_AGENT_ENABLE_LLM_REASONING=${OPS_AGENT_ENABLE_LLM_REASONING:-true}
      - NARRATIVE_VERSION=${NARRATIVE_VERSION:-v1}
      - OPS_AGENT_COUNTER_EVIDENCE_ENABLED=${OPS_AGENT_COUNTER_EVIDENCE_ENABLED:-false}
      - OPS_AGENT_CONFLICT_MATRIX_ENABLED=${OPS_AGENT_CONFLICT_MATRIX_ENABLED:-false}
      - OPS_AGENT_EXPLANATION_BUILDER_ENABLED=${OPS_AGENT_EXPLANATION_BUILDER_ENABLED:-false}
    depends_on:
      - postgres
      - ollama  # NEW: Required for embeddings
```

### Ollama Service

```yaml
# docker-compose.yml

services:
  ollama:
    image: ollama/ollama:latest
    container_name: card-fraud-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_MODELS=mxbai-embed-large,llama3.2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - card-fraud-network

volumes:
  ollama_data:
```

---

## Appendix B: Sample Outputs

### Similarity Search Output

```json
{
  "similarity_result": {
    "matches": [
      {
        "match_id": "tx_001",
        "match_type": "vector",
        "similarity_score": 0.85,
        "details": {
          "card_id": "card_123",
          "merchant_id": "merchant_456",
          "amount": 25.00,
          "decision": "DECLINE"
        },
        "counter_evidence": [
          {
            "type": "3ds_success",
            "strength": 0.8,
            "description": "Transaction had successful 3DS authentication"
          }
        ]
      },
      {
        "match_id": "tx_002",
        "match_type": "attribute",
        "similarity_score": 0.72,
        "details": {
          "card_id": "card_123",
          "merchant_id": "merchant_789",
          "amount": 30.00,
          "decision": "DECLINE"
        },
        "counter_evidence": null
      }
    ],
    "overall_score": 0.78,
    "conflict_matrix": {
      "pattern_vs_similarity": "conflicting",
      "fraud_vs_counter_evidence": "conflicting",
      "deterministic_vs_llm": "aligned",
      "overall_conflict_score": 0.67,
      "resolution_strategy": "flag_for_review"
    },
    "vector_search_count": 20,
    "attribute_match_count": 10
  }
}
```

### Explanation Output (Markdown)

```markdown
# Investigation Report

**Transaction ID:** tx_abc123
**Generated:** 2026-02-15T10:30:00Z

## Executive Summary

High-risk transaction showing cross-merchant card testing pattern with 7 similar declines in past 24 hours. Strong counter-evidence from successful 3DS on 2 prior similar transactions requires review.

**Risk Level:** HIGH

## Pattern Analysis

### Detected Patterns

**velocity** (Score: 0.92)
- Unusual cross-merchant burst: 7 transactions in 24h across 5 merchants
- Decline ratio: 86% (6/7 declined)
- Amount cluster: $10-$50

**decline_anomaly** (Score: 0.78)
- High decline rate detected: 86% (baseline: 15%)
- Cross-merchant velocity: abnormal

## Similarity Analysis

### Similarity Score: 0.78

Found **20** similar transactions.

- Transaction `tx_001`
  - Similarity: 0.85
  - Type: vector
  - Counter-Evidence:
    - Transaction had successful 3DS authentication

- Transaction `tx_002`
  - Similarity: 0.72
  - Type: attribute

## Counter-Evidence

### Evidence Reducing Fraud Risk

- **3ds_success** (Strength: 0.80)
  - 2 similar transactions had successful 3DS

## Conflict Resolution

### Conflict Score: 0.67

**Resolution Strategy:** flag_for_review

**Conflicts Detected:**
- Pattern analysis conflicts with similarity results
- Fraud signals conflict with counter-evidence

## Recommended Actions

1. **Flag for human review** due to conflicting evidence
2. Prioritize review based on conflict score
3. Review device reputation history (prior 90 days)
4. Verify 3DS authentication logs for authenticity
```

---

**End of Document**
