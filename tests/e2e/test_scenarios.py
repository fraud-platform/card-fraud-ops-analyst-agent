"""Scenario-based E2E tests for Ops Analyst Agent.

Tests the full agent pipeline against different transaction profiles:
1. CLEAR_FRAUD — Obvious fraud patterns (velocity, geo anomaly, card testing)
2. LIKELY_FRAUD — Subtle signals (new merchant, elevated amount)
3. LEGITIMATE — Normal behavior, trusted merchant, 3DS success
4. EDGE_CASE_EMPTY_HISTORY — No prior transaction history
5. EDGE_CASE_MISSING_DATA — Null/missing optional fields

Usage:
    # Run all scenarios
    doppler run --config local -- uv run pytest tests/e2e/test_scenarios.py -v

    # Run specific scenario
    doppler run --config local -- uv run pytest tests/e2e/test_scenarios.py::test_scenario_clear_fraud -v

Requires:
    - Ops Analyst Agent server running (http://localhost:8003)
    - Transaction Management server running (http://localhost:8002)
    - Test data seeded via scripts/seed_test_scenarios.py
"""

from __future__ import annotations

import json
import math
import os
import time
from dataclasses import dataclass
from enum import StrEnum
from pathlib import Path
from typing import Any
from urllib.parse import urlparse
from uuid import uuid4

import httpx
import pytest

from scripts.docker_guard import assert_local_docker_ops_agent
from tests.e2e.reporter import E2EReporter


def _resolve_base_url() -> str:
    base_url = os.getenv("E2E_BASE_URL", "http://localhost:8003").strip()
    parsed = urlparse(base_url)
    if (parsed.hostname or "").lower() in {"localhost", "127.0.0.1"}:
        assert_local_docker_ops_agent(base_url)
    return base_url


BASE_URL = _resolve_base_url()
API_PREFIX = "/api/v1/ops-agent"
TM_BASE_URL = os.getenv("TM_BASE_URL", "http://localhost:8002")
TIMEOUT = 180
REPORT_PATH = Path("htmlcov/e2e-scenarios-report.html")
VECTOR_ENABLED = os.getenv("VECTOR_ENABLED", "true").strip().lower() in {"1", "true", "yes", "on"}
VECTOR_API_BASE = os.getenv("VECTOR_API_BASE", "http://localhost:11434/api").strip()
VECTOR_MODEL_NAME = os.getenv("VECTOR_MODEL_NAME", "mxbai-embed-large").strip()
VECTOR_DIMENSION = int(os.getenv("VECTOR_DIMENSION", "1024"))
VECTOR_API_KEY = (os.getenv("VECTOR_API_KEY") or os.getenv("OLLAMA_API_KEY") or "").strip()
SEED_CONTEXT_MARKER = "ops-agent-e2e"
SEED_MANIFEST_PATH = Path(os.getenv("E2E_SEED_MANIFEST", "htmlcov/e2e-seed-manifest.json"))


def _load_seed_manifest(path: Path) -> dict[str, str]:
    """Load scenario -> transaction mapping generated by seed_test_scenarios.py."""
    if not path.exists():
        return {}
    try:
        payload = json.loads(path.read_text(encoding="utf-8"))
    except OSError, json.JSONDecodeError:
        return {}
    scenarios = payload.get("scenarios")
    if not isinstance(scenarios, dict):
        return {}
    return {str(k): str(v) for k, v in scenarios.items() if isinstance(v, str)}


def percentile(values: list[float], pct: float) -> float:
    """Compute percentile using nearest-rank method."""
    if not values:
        return 0.0
    ordered = sorted(values)
    index = max(0, min(len(ordered) - 1, math.ceil(len(ordered) * pct) - 1))
    return float(ordered[index])


SEED_MANIFEST = _load_seed_manifest(SEED_MANIFEST_PATH)
SCENARIO_KPI_RESULTS: dict[str, dict[str, Any]] = {}


# ---------------------------------------------------------------------------
# Session-scoped HTML reporter
# ---------------------------------------------------------------------------


@pytest.fixture(scope="session")
def e2e_reporter():
    """Collect request/response data across all scenario tests, write HTML at end."""
    reporter = E2EReporter(title="E2E Scenario Tests")
    yield reporter
    reporter.write_html(REPORT_PATH)
    print(f"\n  HTML Report: {REPORT_PATH.absolute()}")


@pytest.mark.e2e
def test_vector_embedding_preflight(e2e_reporter: E2EReporter):
    """Fail fast if vector embeddings are enabled but provider/model is not usable."""
    if e2e_reporter:
        e2e_reporter.begin_scenario("Vector Embedding Preflight")

    if not VECTOR_ENABLED:
        pytest.fail("VECTOR_ENABLED=false is not allowed for this suite; vector search is required")
    if not VECTOR_API_BASE:
        pytest.fail("VECTOR_ENABLED=true but VECTOR_API_BASE is empty")

    url = f"{VECTOR_API_BASE.rstrip('/')}/embed"
    headers: dict[str, str] = {}
    if VECTOR_API_KEY:
        headers["Authorization"] = f"Bearer {VECTOR_API_KEY}"

    request_body = {"model": VECTOR_MODEL_NAME, "input": "ops-agent e2e vector preflight"}
    start = time.perf_counter()
    response = httpx.post(url, json=request_body, headers=headers, timeout=60)
    elapsed = (time.perf_counter() - start) * 1000

    response_body: dict[str, Any] | None = None
    error_text: str | None = None
    if response.status_code == 200:
        response_body = response.json()
    else:
        error_text = response.text

    if e2e_reporter:
        notes: list[str] = []
        if response.status_code == 200:
            notes.append("[PASS] Vector embedding endpoint reachable")
        else:
            notes.append("[FAIL] Vector embedding endpoint failed")
        e2e_reporter.record_stage(
            stage_name="Vector Embedding Probe",
            status=response.status_code,
            elapsed_ms=elapsed,
            request_method="POST",
            request_url=url,
            request_body={"model": VECTOR_MODEL_NAME, "input": "[redacted test input]"},
            response_status=response.status_code,
            response_body=response_body,
            error=error_text,
            notes=notes,
        )

    assert response.status_code == 200, (
        f"Vector embedding preflight failed ({response.status_code}): {response.text}"
    )

    payload = response.json()
    embeddings = payload.get("embeddings")
    assert isinstance(embeddings, list) and embeddings, "Embedding response missing embeddings list"
    first_embedding = embeddings[0]
    assert isinstance(first_embedding, list) and first_embedding, "Embedding vector missing"
    assert len(first_embedding) == VECTOR_DIMENSION, (
        f"Embedding dimension mismatch: expected {VECTOR_DIMENSION}, got {len(first_embedding)}"
    )


@pytest.mark.e2e
def test_seed_manifest_preflight(e2e_reporter: E2EReporter):
    """Fail fast when deterministic seed manifest is missing or incomplete."""
    if e2e_reporter:
        e2e_reporter.begin_scenario("Seed Manifest Preflight")
    required = {scenario.value for scenario in FraudScenario}
    observed = set(SEED_MANIFEST)
    missing = sorted(required - observed)
    notes = [
        f"[PASS] Loaded seed manifest: {SEED_MANIFEST_PATH}",
        f"[PASS] Seeded scenarios: {len(observed)}",
    ]
    if missing:
        notes.append(f"[FAIL] Missing scenarios in manifest: {', '.join(missing)}")
    if e2e_reporter:
        e2e_reporter.record_stage(
            stage_name="Validate Seed Manifest",
            status=200 if not missing and bool(SEED_MANIFEST) else 400,
            elapsed_ms=0,
            request_method="INTERNAL",
            request_url=str(SEED_MANIFEST_PATH),
            response_status=200 if not missing and bool(SEED_MANIFEST) else 400,
            response_body={
                "manifest_path": str(SEED_MANIFEST_PATH),
                "loaded": bool(SEED_MANIFEST),
                "scenario_count": len(observed),
                "missing": missing,
            },
            notes=notes,
        )
    assert SEED_MANIFEST, (
        "Seed manifest missing or unreadable. Run: doppler run --config local -- uv run "
        "python scripts/seed_test_scenarios.py"
    )
    assert not missing, f"Seed manifest missing scenarios: {', '.join(missing)}"


class FraudScenario(StrEnum):
    """Test scenario types with expected outcomes."""

    # Specific fraud patterns
    CARD_TESTING_PATTERN = "card_testing_pattern"
    VELOCITY_BURST = "velocity_burst"
    CROSS_MERCHANT_SPREAD = "cross_merchant_spread"
    HIGH_DECLINE_RATIO = "high_decline_ratio"

    # Medium confidence — mixed signals
    LIKELY_FRAUD = "likely_fraud"

    # Low confidence — normal patterns
    LEGITIMATE = "legitimate"

    # Legitimate with counter-evidence (should downgrade severity)
    LEGITIMATE_WITH_COUNTER_EVIDENCE = "legitimate_counter_evidence"

    # Approved transaction but flagged as suspicious by pipeline
    APPROVED_LIKELY_FRAUD = "approved_likely_fraud"

    # Edge case: first transaction on card
    EDGE_FIRST_TRANSACTION = "edge_first_transaction"

    # Edge case: missing optional fields
    EDGE_MISSING_DATA = "edge_missing_data"

    # New patterns - Amount anomaly
    AMOUNT_ROUND_NUMBER = "amount_round_number"
    AMOUNT_HIGH = "amount_high"

    # New patterns - Time anomaly
    TIME_UNUSUAL_HOUR = "time_unusual_hour"
    TIMEZONE_MISMATCH = "timezone_mismatch"

    # New patterns - Extended counter-evidence
    COUNTER_EVIDENCE_EXTENDED = "counter_evidence_extended"

    # New patterns - Card testing sequence
    CARD_TESTING_SEQUENCE = "card_testing_sequence"


@dataclass
class ScenarioExpectations:
    """Expected outcomes for a fraud scenario."""

    severity_min: str  # Minimum expected severity (LOW, MEDIUM, HIGH, CRITICAL)
    min_recommendations: int  # Minimum number of recommendations
    should_have_evidence: bool  # Should have structured evidence
    expected_keywords: list[str]  # Keywords that should appear in summary


# Scenario expectations based on fraud patterns
SCENARIO_EXPECTATIONS: dict[FraudScenario, ScenarioExpectations] = {
    # Specific fraud patterns with proper thresholds
    FraudScenario.CARD_TESTING_PATTERN: ScenarioExpectations(
        severity_min="MEDIUM",  # Seeded burst should trigger clear suspicious indicators
        min_recommendations=2,
        should_have_evidence=False,
        expected_keywords=["card testing", "velocity", "decline", "multiple merchants"],
    ),
    FraudScenario.VELOCITY_BURST: ScenarioExpectations(
        severity_min="HIGH",
        min_recommendations=2,
        should_have_evidence=False,
        expected_keywords=["velocity", "burst", "high frequency", "unusual pattern"],
    ),
    FraudScenario.CROSS_MERCHANT_SPREAD: ScenarioExpectations(
        severity_min="MEDIUM",
        min_recommendations=1,
        should_have_evidence=False,
        expected_keywords=["cross merchant", "multiple merchants", "unusual spread"],
    ),
    FraudScenario.HIGH_DECLINE_RATIO: ScenarioExpectations(
        severity_min="MEDIUM",
        min_recommendations=2,
        should_have_evidence=False,
        expected_keywords=["decline ratio", "high decline rate", "anomaly"],
    ),
    FraudScenario.LIKELY_FRAUD: ScenarioExpectations(
        severity_min="LOW",
        min_recommendations=1,
        should_have_evidence=False,
        expected_keywords=["elevated", "merchant", "review"],
    ),
    FraudScenario.LEGITIMATE: ScenarioExpectations(
        severity_min="LOW",
        min_recommendations=0,
        should_have_evidence=False,
        expected_keywords=["normal", "trusted", "no anomalies"],
    ),
    FraudScenario.LEGITIMATE_WITH_COUNTER_EVIDENCE: ScenarioExpectations(
        severity_min="LOW",
        min_recommendations=0,
        should_have_evidence=False,  # Evidence builder is feature-flagged, may not be enabled
        expected_keywords=["3ds", "trusted device", "counter-evidence", "downgrade"],
    ),
    FraudScenario.APPROVED_LIKELY_FRAUD: ScenarioExpectations(
        severity_min="LOW",
        min_recommendations=1,
        should_have_evidence=False,
        expected_keywords=["approved", "suspicious", "review", "flagged"],
    ),
    FraudScenario.EDGE_FIRST_TRANSACTION: ScenarioExpectations(
        severity_min="LOW",
        min_recommendations=0,
        should_have_evidence=False,  # Structured evidence may be disabled in deployed envs
        expected_keywords=["first", "new", "limited history"],
    ),
    FraudScenario.EDGE_MISSING_DATA: ScenarioExpectations(
        severity_min="LOW",
        min_recommendations=0,
        should_have_evidence=False,  # Structured evidence may be disabled in deployed envs
        expected_keywords=["missing", "incomplete", "limited data"],
    ),
    FraudScenario.AMOUNT_ROUND_NUMBER: ScenarioExpectations(
        # A single round-number amount anomaly may remain LOW if other risk factors are absent.
        severity_min="LOW",
        min_recommendations=1,
        should_have_evidence=False,
        expected_keywords=["amount", "round", "anomaly", "$500"],
    ),
    FraudScenario.AMOUNT_HIGH: ScenarioExpectations(
        # A single elevated amount signal can remain LOW without corroborating velocity/similarity evidence.
        severity_min="LOW",
        min_recommendations=1,
        should_have_evidence=False,
        expected_keywords=["amount", "high", "elevated"],
    ),
    FraudScenario.TIME_UNUSUAL_HOUR: ScenarioExpectations(
        # Unusual-hour alone is advisory unless reinforced by additional indicators.
        severity_min="LOW",
        min_recommendations=1,
        should_have_evidence=False,
        expected_keywords=["time", "hour", "unusual", "3 AM"],
    ),
    FraudScenario.TIMEZONE_MISMATCH: ScenarioExpectations(
        # Timezone mismatch can be advisory-only (LOW) unless corroborated by additional signals.
        severity_min="LOW",
        min_recommendations=1,
        should_have_evidence=False,
        expected_keywords=["timezone", "country", "mismatch"],
    ),
    FraudScenario.COUNTER_EVIDENCE_EXTENDED: ScenarioExpectations(
        severity_min="LOW",
        min_recommendations=0,
        should_have_evidence=False,
        expected_keywords=["avs", "cvv", "tokenized", "counter-evidence"],
    ),
    FraudScenario.CARD_TESTING_SEQUENCE: ScenarioExpectations(
        severity_min="MEDIUM",
        min_recommendations=1,
        should_have_evidence=False,
        expected_keywords=["card testing", "increasing", "sequence"],
    ),
}


SEVERITY_ORDER = {"LOW": 1, "MEDIUM": 2, "HIGH": 3, "CRITICAL": 4}
LOW_RISK_SCENARIOS = {
    FraudScenario.LEGITIMATE,
    FraudScenario.LEGITIMATE_WITH_COUNTER_EVIDENCE,
    FraudScenario.EDGE_FIRST_TRANSACTION,
    FraudScenario.EDGE_MISSING_DATA,
}
FRAUD_RISK_SCENARIOS = {
    FraudScenario.CARD_TESTING_PATTERN,
    FraudScenario.VELOCITY_BURST,
    FraudScenario.CROSS_MERCHANT_SPREAD,
    FraudScenario.HIGH_DECLINE_RATIO,
    FraudScenario.LIKELY_FRAUD,
    FraudScenario.APPROVED_LIKELY_FRAUD,
}
# High-confidence fraud scenarios used for recall KPI.
# Excludes mixed-signal cases (e.g., likely/approved-likely) that are advisory and may remain LOW.
FRAUD_RECALL_SCENARIOS = {
    FraudScenario.CARD_TESTING_PATTERN,
    FraudScenario.VELOCITY_BURST,
    FraudScenario.CROSS_MERCHANT_SPREAD,
    FraudScenario.HIGH_DECLINE_RATIO,
}
ACCEPTANCE_KPI_THRESHOLDS: dict[str, float] = {
    "scenario_pass_rate": 1.0,
    "fraud_recall_medium_plus": 0.80,
    "low_risk_precision_low_only": 1.0,
    "recommendation_coverage": 1.0,
    # Hybrid pipeline includes external LLM calls and deterministic fallback when LLM times out.
    # Keep a realistic upper bound for end-to-end investigation latency in this environment.
    "run_investigation_p95_ms": 30000.0,
    "detail_fetch_p95_ms": 4000.0,
}


def severity_gte(severity: str, min_severity: str) -> bool:
    """Check if severity is greater than or equal to minimum."""
    return SEVERITY_ORDER.get(severity, 0) >= SEVERITY_ORDER.get(min_severity, 0)


def compute_acceptance_kpis(results: dict[str, dict[str, Any]]) -> dict[str, dict[str, Any]]:
    """Compute suite-level acceptance KPIs from per-scenario run results."""
    rows = list(results.values())
    if not rows:
        return {}

    def ratio(num: int, den: int) -> float:
        return round((num / den), 3) if den else 0.0

    pass_count = sum(1 for row in rows if row.get("passed"))
    fraud_rows = [
        row for row in rows if row.get("scenario") in {s.value for s in FRAUD_RECALL_SCENARIOS}
    ]
    low_rows = [row for row in rows if row.get("scenario") in {s.value for s in LOW_RISK_SCENARIOS}]
    recommendation_rows = [row for row in rows if int(row.get("min_recommendations", 0)) > 0]

    fraud_detected_medium_plus = sum(
        1 for row in fraud_rows if severity_gte(str(row.get("severity", "LOW")), "MEDIUM")
    )
    low_risk_kept_low = sum(1 for row in low_rows if str(row.get("severity", "LOW")) == "LOW")
    recommendation_met = sum(
        1
        for row in recommendation_rows
        if int(row.get("recommendations_count", 0)) >= int(row.get("min_recommendations", 0))
    )

    run_latencies = [
        float(row["run_investigation_ms"])
        for row in rows
        if row.get("run_investigation_ms") is not None
    ]
    detail_latencies = [
        float(row["get_investigation_detail_ms"])
        for row in rows
        if row.get("get_investigation_detail_ms") is not None
    ]

    scenario_pass_rate = ratio(pass_count, len(rows))
    fraud_recall = ratio(fraud_detected_medium_plus, len(fraud_rows))
    low_risk_precision = ratio(low_risk_kept_low, len(low_rows))
    recommendation_coverage = ratio(recommendation_met, len(recommendation_rows))
    run_p95 = round(percentile(run_latencies, 0.95), 1) if run_latencies else 0.0
    detail_p95 = round(percentile(detail_latencies, 0.95), 1) if detail_latencies else 0.0

    return {
        "scenario_pass_rate": {
            "value": scenario_pass_rate,
            "target": ACCEPTANCE_KPI_THRESHOLDS["scenario_pass_rate"],
            "pass": scenario_pass_rate >= ACCEPTANCE_KPI_THRESHOLDS["scenario_pass_rate"],
            "description": f"{pass_count}/{len(rows)} scenarios passed",
        },
        "fraud_recall_medium_plus": {
            "value": fraud_recall,
            "target": ACCEPTANCE_KPI_THRESHOLDS["fraud_recall_medium_plus"],
            "pass": fraud_recall >= ACCEPTANCE_KPI_THRESHOLDS["fraud_recall_medium_plus"],
            "description": f"{fraud_detected_medium_plus}/{len(fraud_rows)} fraud scenarios at MEDIUM+",
        },
        "low_risk_precision_low_only": {
            "value": low_risk_precision,
            "target": ACCEPTANCE_KPI_THRESHOLDS["low_risk_precision_low_only"],
            "pass": low_risk_precision >= ACCEPTANCE_KPI_THRESHOLDS["low_risk_precision_low_only"],
            "description": f"{low_risk_kept_low}/{len(low_rows)} low-risk scenarios remained LOW",
        },
        "recommendation_coverage": {
            "value": recommendation_coverage,
            "target": ACCEPTANCE_KPI_THRESHOLDS["recommendation_coverage"],
            "pass": recommendation_coverage >= ACCEPTANCE_KPI_THRESHOLDS["recommendation_coverage"],
            "description": (
                f"{recommendation_met}/{len(recommendation_rows)} scenarios met recommendation minimums"
            ),
        },
        "run_investigation_p95_ms": {
            "value": run_p95,
            "target": ACCEPTANCE_KPI_THRESHOLDS["run_investigation_p95_ms"],
            "pass": run_p95 <= ACCEPTANCE_KPI_THRESHOLDS["run_investigation_p95_ms"],
            "description": f"p95 run latency over {len(run_latencies)} samples",
        },
        "detail_fetch_p95_ms": {
            "value": detail_p95,
            "target": ACCEPTANCE_KPI_THRESHOLDS["detail_fetch_p95_ms"],
            "pass": detail_p95 <= ACCEPTANCE_KPI_THRESHOLDS["detail_fetch_p95_ms"],
            "description": f"p95 detail latency over {len(detail_latencies)} samples",
        },
    }


class ScenarioTestRunner:
    """Test runner for fraud scenarios."""

    def __init__(
        self,
        scenario: FraudScenario,
        reporter: E2EReporter | None = None,
        server_features: dict[str, bool] | None = None,
    ):
        self.scenario = scenario
        self.expectations = SCENARIO_EXPECTATIONS[scenario]
        self.client = httpx.Client(base_url=BASE_URL, timeout=TIMEOUT)
        self.tm_client = httpx.Client(base_url=TM_BASE_URL, timeout=TIMEOUT)
        self.server_features = server_features or self._fetch_server_features()
        # Use unique case_id so each run is fresh and demo output isn't tied to stale investigations.
        self.case_id = f"e2e-{scenario.value}-{int(time.time() * 1000)}-{uuid4().hex[:8]}"
        self.results: dict[str, Any] = {}
        self.timings: dict[str, float] = {}
        self._reporter = reporter
        if reporter:
            reporter.begin_scenario(str(scenario))

    def _fetch_server_features(self) -> dict[str, bool]:
        """Read feature flags from readiness endpoint for dynamic E2E assertions."""
        try:
            response = self.client.get("/api/v1/health/ready", timeout=10)
        except httpx.HTTPError:
            return {}
        if response.status_code != 200:
            return {}
        payload = response.json()
        features = payload.get("features")
        if not isinstance(features, dict):
            return {}
        return {str(k): bool(v) for k, v in features.items()}

    def close(self):
        """Close HTTP clients."""
        self.client.close()
        self.tm_client.close()

    def log(self, stage: str, message: str):
        """Log a test stage message."""
        print(f"[{self.scenario}] {stage}: {message}")

    @staticmethod
    def _parse_strength(value: Any) -> float:
        """Parse evidence strength into a float for report rendering."""
        if isinstance(value, (int, float)):
            return float(value)
        try:
            return float(value)
        except TypeError, ValueError:
            return 0.0

    @classmethod
    def _summarize_evidence_item(cls, evidence_item: Any) -> dict[str, Any]:
        """Normalize evidence shapes from API payloads for analyst-facing summaries."""
        if not isinstance(evidence_item, dict):
            return {"kind": "", "category": "", "strength": 0.0, "description": ""}

        payload = evidence_item.get("evidence_payload")
        payload_dict = payload if isinstance(payload, dict) else {}

        kind = str(
            evidence_item.get("evidence_kind")
            or evidence_item.get("kind")
            or payload_dict.get("evidence_kind")
            or payload_dict.get("kind")
            or ""
        )
        category = str(evidence_item.get("category") or payload_dict.get("category") or "")
        strength = cls._parse_strength(
            evidence_item.get("strength", payload_dict.get("strength", 0))
        )
        description = str(
            evidence_item.get("description") or payload_dict.get("description") or ""
        )[:200]

        return {
            "kind": kind,
            "category": category,
            "strength": round(strength, 3),
            "description": description,
        }

    @classmethod
    def _build_similarity_snapshot(
        cls,
        detail: dict[str, Any],
        evidence: list[Any],
    ) -> tuple[dict[str, Any], list[str]]:
        """Build analyst-friendly similarity diagnostics for the HTML report."""
        stage_durations = detail.get("stage_durations")
        durations = stage_durations if isinstance(stage_durations, dict) else {}
        similarity_stage_ms = round(cls._parse_strength(durations.get("similarity_analysis")), 1)

        matches: list[dict[str, Any]] = []
        for raw_item in evidence:
            if not isinstance(raw_item, dict):
                continue
            summarized = cls._summarize_evidence_item(raw_item)
            payload = raw_item.get("evidence_payload")
            payload_dict = payload if isinstance(payload, dict) else {}
            supporting_data = payload_dict.get("supporting_data")
            support = supporting_data if isinstance(supporting_data, dict) else {}

            match_id = str(
                support.get("match_id")
                or (payload_dict.get("related_transaction_ids") or [None])[0]
                or ""
            )
            match_type = str(support.get("match_type") or summarized.get("category") or "")
            score = round(
                cls._parse_strength(support.get("similarity_score", summarized.get("strength", 0))),
                3,
            )

            if (
                "similar"
                not in (summarized.get("kind", "") + summarized.get("description", "")).lower()
            ):
                continue

            matches.append(
                {
                    "match_id": match_id,
                    "match_type": match_type or "unknown",
                    "score": score,
                    "counter_evidence": support.get("counter_evidence"),
                }
            )

        matches.sort(key=lambda item: item["score"], reverse=True)

        snapshot = {
            "vector_enabled": VECTOR_ENABLED,
            "similarity_stage_ms": similarity_stage_ms,
            "similarity_match_count": len(matches),
            "top_similarity_matches": matches[:5],
            "stage_durations": durations,
        }

        notes: list[str] = []
        if similarity_stage_ms > 0:
            notes.append(f"[PASS] Similarity stage duration: {similarity_stage_ms}ms")
        else:
            notes.append("[WARN] Similarity stage duration missing from detail payload")
        if matches:
            notes.append(f"[PASS] Similarity matches found: {len(matches)}")
        else:
            notes.append("[WARN] Similarity matches not present in evidence for this run")

        return snapshot, notes

    def find_transaction(self) -> str | None:
        """Find a transaction matching the scenario criteria.

        Returns transaction_id or None if no match found.
        Uses merchant_name pattern matching and decision filters to find seeded test data.
        """
        self.log("SETUP", f"Finding transaction for scenario: {self.scenario}")

        manifest_txn_id = SEED_MANIFEST.get(self.scenario.value)
        if manifest_txn_id:
            if self._reporter:
                self._reporter.record_stage(
                    stage_name="Find Transaction (Seed Manifest)",
                    status=200,
                    elapsed_ms=0,
                    request_method="INTERNAL",
                    request_url=str(SEED_MANIFEST_PATH),
                    response_status=200,
                    response_body={
                        "scenario": self.scenario.value,
                        "transaction_id": manifest_txn_id,
                        "manifest_path": str(SEED_MANIFEST_PATH),
                    },
                    notes=["[PASS] Using deterministic seeded transaction from manifest"],
                )
            self.log("SETUP", f"Using seeded transaction from manifest: {manifest_txn_id}")
            return manifest_txn_id

        params: dict[str, Any] = {"limit": 500}

        url = f"{TM_BASE_URL}/api/v1/transactions"
        start = time.perf_counter()
        response = self.tm_client.get("/api/v1/transactions", params=params)
        elapsed = (time.perf_counter() - start) * 1000

        if response.status_code != 200:
            if self._reporter:
                self._reporter.record_stage(
                    stage_name="Find Transaction (TM)",
                    status=response.status_code,
                    elapsed_ms=elapsed,
                    request_method="GET",
                    request_url=f"{url}?{'&'.join(f'{k}={v}' for k, v in params.items())}",
                    response_status=response.status_code,
                    error=response.text,
                )
            self.log("SETUP", f"TM API error: {response.status_code}")
            raise AssertionError(f"TM API error: {response.status_code}")

        transactions = response.json().get("items", [])
        transactions.sort(key=lambda t: str(t.get("transaction_timestamp", "")), reverse=True)
        self.log("SETUP", f"TM returned {len(transactions)} transactions")

        selected_txn: dict[str, Any] | None = None
        for txn in transactions:
            txn_id = txn.get("transaction_id")
            merchant_id = txn.get("merchant_id", "")
            card_id = txn.get("card_id", "")
            transaction_context = txn.get("transaction_context") or {}

            if isinstance(transaction_context, dict):
                if (
                    transaction_context.get("seed_marker") == SEED_CONTEXT_MARKER
                    and transaction_context.get("seed_scenario") == self.scenario.value
                    and bool(transaction_context.get("seed_is_target"))
                ):
                    self.log("SETUP", f"Found scenario target via seed marker: {txn_id}")
                    selected_txn = txn
                    break

            if self.scenario == FraudScenario.CARD_TESTING_PATTERN:
                # Match by merchant_id pattern "test-merchant-cardtest-X"
                if "test-merchant-cardtest" in merchant_id:
                    self.log("SETUP", f"Found CARD_TESTING_PATTERN transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.VELOCITY_BURST:
                # Match by merchant_id containing "test-merchant-velocityburst"
                if "test-merchant-velocityburst" in merchant_id:
                    self.log("SETUP", f"Found VELOCITY_BURST transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.CROSS_MERCHANT_SPREAD:
                # Match by merchant_id containing "test-merchant-cross"
                if "test-merchant-cross" in merchant_id:
                    self.log("SETUP", f"Found CROSS_MERCHANT_SPREAD transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.HIGH_DECLINE_RATIO:
                # Match by merchant_id containing "test-merchant-declineratio"
                if "test-merchant-declineratio" in merchant_id:
                    self.log("SETUP", f"Found HIGH_DECLINE_RATIO transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.LIKELY_FRAUD:
                # Match by merchant_id containing "test-merchant-likelyfraud"
                if "test-merchant-likelyfraud" in merchant_id:
                    self.log("SETUP", f"Found LIKELY_FRAUD transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.LEGITIMATE:
                # Match by merchant_id containing "test-merchant-legitimate"
                if "test-merchant-legitimate" in merchant_id:
                    self.log("SETUP", f"Found LEGITIMATE transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.LEGITIMATE_WITH_COUNTER_EVIDENCE:
                # Match by merchant_id containing "test-merchant-counterevidence"
                if "test-merchant-counterevidence" in merchant_id:
                    self.log(
                        "SETUP", f"Found LEGITIMATE_WITH_COUNTER_EVIDENCE transaction: {txn_id}"
                    )
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.APPROVED_LIKELY_FRAUD:
                # Match by merchant_id containing "test-merchant-approvedfraud"
                if "test-merchant-approvedfraud" in merchant_id:
                    self.log("SETUP", f"Found APPROVED_LIKELY_FRAUD transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.EDGE_FIRST_TRANSACTION:
                # Match by card_id prefix "tok_first_"
                if card_id.startswith("tok_first_"):
                    self.log("SETUP", f"Found EDGE_FIRST_TRANSACTION transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.EDGE_MISSING_DATA:
                merchant_id = txn.get("merchant_id", "")
                if "unknown" in merchant_id.lower() or not transaction_context:
                    self.log("SETUP", f"Found EDGE_MISSING_DATA transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.AMOUNT_ROUND_NUMBER:
                if card_id.startswith("tok_round_"):
                    self.log("SETUP", f"Found AMOUNT_ROUND_NUMBER transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.AMOUNT_HIGH:
                if card_id.startswith("tok_highamt_"):
                    self.log("SETUP", f"Found AMOUNT_HIGH transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.TIME_UNUSUAL_HOUR:
                if card_id.startswith("tok_latenight_"):
                    self.log("SETUP", f"Found TIME_UNUSUAL_HOUR transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.TIMEZONE_MISMATCH:
                if card_id.startswith("tok_tz_"):
                    self.log("SETUP", f"Found TIMEZONE_MISMATCH transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.COUNTER_EVIDENCE_EXTENDED:
                if card_id.startswith("tok_extcounter_"):
                    self.log("SETUP", f"Found COUNTER_EVIDENCE_EXTENDED transaction: {txn_id}")
                    selected_txn = txn
                    break

            elif self.scenario == FraudScenario.CARD_TESTING_SEQUENCE:
                if card_id.startswith("tok_cardtestseq_"):
                    self.log("SETUP", f"Found CARD_TESTING_SEQUENCE transaction: {txn_id}")
                    selected_txn = txn
                    break

        if self._reporter:
            if selected_txn:
                # Show the selected transaction's full details so the report is useful
                self._reporter.record_stage(
                    stage_name="Find Transaction (TM)",
                    status=200,
                    elapsed_ms=elapsed,
                    request_method="GET",
                    request_url=f"{url}?{'&'.join(f'{k}={v}' for k, v in params.items())}",
                    response_status=200,
                    response_body={
                        "selected": selected_txn,
                        "pool_total": response.json().get("total"),
                        "pool_returned": len(transactions),
                    },
                    notes=[f"[PASS] Selected transaction: {selected_txn.get('transaction_id')}"],
                )
            else:
                self._reporter.record_stage(
                    stage_name="Find Transaction (TM)",
                    status=200,
                    elapsed_ms=elapsed,
                    request_method="GET",
                    request_url=f"{url}?{'&'.join(f'{k}={v}' for k, v in params.items())}",
                    response_status=200,
                    response_body={
                        "selected": None,
                        "pool_total": response.json().get("total"),
                        "pool_returned": len(transactions),
                    },
                    notes=["[WARN] No matching transaction found - scenario will fail"],
                )

        if not selected_txn:
            self.log("SETUP", f"No matching transaction found for {self.scenario}")
            return None

        return selected_txn.get("transaction_id")

    def run_investigation(self, transaction_id: str) -> dict[str, Any]:
        """Run the investigation pipeline.

        Handles 409 Conflict gracefully: if already investigated, fetches
        the existing run_id from the response and proceeds with it.
        """
        self.log("RUN", f"Starting investigation for {transaction_id}")

        url = f"{API_PREFIX}/investigations/run"
        req_body = {
            "transaction_id": transaction_id,
            "mode": "quick",
            "case_id": self.case_id,
        }
        start = time.perf_counter()
        response = self.client.post(url, json=req_body)
        elapsed = (time.perf_counter() - start) * 1000
        self.timings["run_investigation_ms"] = elapsed

        self.log("RUN", f"HTTP {response.status_code} ({elapsed:.0f}ms)")

        if response.status_code == 409:
            body = response.json()
            errors = body.get("errors", {})
            detail = body.get("detail", {})
            run_id = (errors.get("run_id") if isinstance(errors, dict) else None) or (
                detail.get("run_id") if isinstance(detail, dict) else None
            )
            if self._reporter:
                self._reporter.record_stage(
                    stage_name="Run Investigation",
                    status=200,  # treat reuse as pass
                    elapsed_ms=elapsed,
                    request_method="POST",
                    request_url=f"{BASE_URL}{url}",
                    request_body=req_body,
                    response_status=409,
                    response_body=body,
                    notes=["[PASS] Already investigated — reusing existing run_id"],
                )
            if run_id:
                self.log("RUN", f"Already investigated, reusing run_id={run_id}")
                return {"run_id": run_id, "model_mode": "deterministic"}
            raise AssertionError(f"Run returned 409 but no run_id in response: {response.text}")

        if self._reporter:
            self._reporter.record_stage(
                stage_name="Run Investigation",
                status=response.status_code,
                elapsed_ms=elapsed,
                request_method="POST",
                request_url=f"{BASE_URL}{url}",
                request_body=req_body,
                response_status=response.status_code,
                response_body=response.json() if response.status_code == 200 else None,
                error=response.text if response.status_code != 200 else None,
            )

        if response.status_code != 200:
            self.log("RUN", f"Failed: {response.text}")
            raise AssertionError(f"Run investigation failed: {response.status_code}")

        data = response.json()
        self.log("RUN", f"run_id={data.get('run_id')}, mode={data.get('model_mode')}")
        return data

    def get_investigation_detail(self, run_id: str) -> dict[str, Any]:
        """Get investigation detail."""
        self.log("DETAIL", f"Fetching investigation {run_id}")

        url = f"{API_PREFIX}/investigations/{run_id}"
        max_attempts = 4
        retry_delays = (0.25, 0.5, 1.0)
        response: httpx.Response | None = None
        start = time.perf_counter()
        for attempt in range(1, max_attempts + 1):
            response = self.client.get(url)
            self.log("DETAIL", f"Attempt {attempt}/{max_attempts}: HTTP {response.status_code}")
            if response.status_code == 200:
                break
            if response.status_code == 404 and attempt < max_attempts:
                time.sleep(retry_delays[attempt - 1])
                continue
            break
        elapsed = (time.perf_counter() - start) * 1000
        self.timings["get_investigation_detail_ms"] = elapsed
        assert response is not None

        self.log("DETAIL", f"HTTP {response.status_code} ({elapsed:.0f}ms)")

        if self._reporter:
            notes: list[str] = []
            if response.status_code == 200 and attempt > 1:
                notes.append(
                    f"[PASS] Investigation detail became available after {attempt} attempts"
                )
            self._reporter.record_stage(
                stage_name="Get Investigation Detail",
                status=response.status_code,
                elapsed_ms=elapsed,
                request_method="GET",
                request_url=f"{BASE_URL}{url}",
                response_status=response.status_code,
                response_body=response.json() if response.status_code == 200 else None,
                error=response.text if response.status_code != 200 else None,
                notes=notes,
            )

        if response.status_code != 200:
            raise AssertionError(f"Get investigation failed: {response.status_code}")

        return response.json()

    def validate_expectations(self, detail: dict[str, Any]) -> bool:
        """Validate results against scenario expectations."""
        insight = detail.get("insight") or {}
        recommendations = detail.get("recommendations", [])
        evidence = detail.get("evidence", [])

        severity = insight.get("severity", "LOW")
        summary = insight.get("summary", "")
        recommendation_text = " ".join(
            f"{(rec.get('payload') or {}).get('title', '')} {(rec.get('payload') or {}).get('impact', '')}"
            for rec in recommendations
            if isinstance(rec, dict)
        )
        # pattern_score is not exposed in the API response - severity captures the result
        # pattern_score = float(insight.get("pattern_score") or 0.0)

        passed = True
        severity_pass = True
        recommendation_pass = True
        evidence_pass = True
        notes: list[str] = []

        # Check severity
        if not severity_gte(severity, self.expectations.severity_min):
            msg = f"[FAIL] Severity {severity} < expected {self.expectations.severity_min}"
            self.log("VALIDATE", msg)
            notes.append(msg)
            severity_pass = False
            passed = False
        else:
            msg = f"[PASS] Severity {severity} meets minimum"
            self.log("VALIDATE", msg)
            notes.append(msg)

        if self.scenario in LOW_RISK_SCENARIOS and severity != "LOW":
            msg = f"[FAIL] Low-risk scenario escalated unexpectedly: severity={severity}"
            self.log("VALIDATE", msg)
            notes.append(msg)
            severity_pass = False
            passed = False
        elif self.scenario in LOW_RISK_SCENARIOS:
            msg = "[PASS] Low-risk scenario remained LOW severity"
            self.log("VALIDATE", msg)
            notes.append(msg)

        if self.scenario in FRAUD_RISK_SCENARIOS:
            severity_floor = SEVERITY_ORDER.get(self.expectations.severity_min, 0)
            if severity in {"CRITICAL", "HIGH"}:
                msg = f"[PASS] Fraud identified with {severity} severity"
            elif severity == "MEDIUM":
                msg = "[PASS] Fraud indicators identified; manual review required"
            elif severity_floor >= SEVERITY_ORDER["MEDIUM"]:
                msg = "[WARN] Scenario classified as LOW risk despite fraud seed profile"
            else:
                msg = "[PASS] Advisory fraud-seed scenario allowed to remain LOW"
            self.log("VALIDATE", msg)
            notes.append(msg)
        elif self.scenario == FraudScenario.LEGITIMATE_WITH_COUNTER_EVIDENCE and severity == "LOW":
            msg = "[PASS] Counter-evidence lowered severity to LOW as expected"
            self.log("VALIDATE", msg)
            notes.append(msg)

        # Check recommendations count
        if len(recommendations) < self.expectations.min_recommendations:
            msg = f"[FAIL] Recommendations {len(recommendations)} < expected {self.expectations.min_recommendations}"
            self.log("VALIDATE", msg)
            notes.append(msg)
            recommendation_pass = False
            passed = False
        else:
            msg = f"[PASS] Recommendations {len(recommendations)} meets minimum"
            self.log("VALIDATE", msg)
            notes.append(msg)

        expect_evidence = self.expectations.should_have_evidence or any(
            self.server_features.get(flag, False)
            for flag in ("counter_evidence_enabled", "conflict_matrix_enabled")
        )

        # Check evidence presence
        if expect_evidence and not evidence:
            msg = "[FAIL] Expected evidence but found none"
            self.log("VALIDATE", msg)
            notes.append(msg)
            evidence_pass = False
            passed = False
        else:
            msg = f"[PASS] Evidence: {len(evidence)} items"
            if self.server_features:
                msg += f" | features={self.server_features}"
            self.log("VALIDATE", msg)
            notes.append(msg)

        # Note: pattern_score validation removed - not exposed in API response
        # Severity captures the overall pattern analysis result

        # Check keywords (warning only)
        summary_lower = f"{summary} {recommendation_text}".lower()
        missing_keywords = [
            kw for kw in self.expectations.expected_keywords if kw.lower() not in summary_lower
        ]
        advisory_fraud_seed = (
            self.scenario in FRAUD_RISK_SCENARIOS
            and SEVERITY_ORDER.get(self.expectations.severity_min, 0) <= SEVERITY_ORDER["LOW"]
        )
        suppress_keyword_warning = (
            self.scenario in FRAUD_RISK_SCENARIOS and severity in {"HIGH", "CRITICAL"}
        ) or advisory_fraud_seed
        if missing_keywords and not suppress_keyword_warning:
            msg = f"[WARN] Missing keywords: {', '.join(missing_keywords)}"
            self.log("VALIDATE", msg)
            notes.append(msg)

        if self._reporter:
            agentic_trace = detail.get("agentic_trace", {})
            trace_notes: list[str] = []
            llm_status = (
                agentic_trace.get("llm_status")
                if isinstance(agentic_trace, dict)
                else detail.get("llm_status")
            )
            if llm_status:
                trace_notes.append(f"[PASS] LLM status recorded: {llm_status}")
            if isinstance(agentic_trace, dict) and agentic_trace.get("llm_reasoning_hash"):
                trace_notes.append("[PASS] LLM reasoning hash present for audit correlation")
            if isinstance(agentic_trace, dict) and agentic_trace.get("stages"):
                trace_notes.append("[PASS] Stage-level agentic trace available")
            self._reporter.record_stage(
                stage_name="Agentic Trace Audit",
                status=200,
                elapsed_ms=0,
                request_method="ANALYSIS",
                request_url="agentic-trace",
                response_body=agentic_trace,
                response_status=200,
                notes=trace_notes,
            )

            similarity_snapshot, similarity_notes = self._build_similarity_snapshot(
                detail, evidence
            )
            self._reporter.record_stage(
                stage_name="Similarity Search Analysis",
                status=200,
                elapsed_ms=0,
                request_method="ANALYSIS",
                request_url="similarity-search",
                response_body=similarity_snapshot,
                response_status=200,
                notes=similarity_notes,
            )

            # Build comprehensive analyst view for the HTML report
            analyst_view: dict[str, Any] = {
                "severity": severity,
                "summary": summary,  # Full summary, not truncated
                "recommendation_count": len(recommendations),
                "recommendations": [
                    {
                        "type": rec.get("type") or rec.get("recommendation_type", ""),
                        "title": (rec.get("payload") or {}).get("title", ""),
                        "impact": (rec.get("payload") or {}).get("impact", ""),
                        "status": rec.get("status", ""),
                    }
                    for rec in recommendations
                    if isinstance(rec, dict)
                ][:10],
                "evidence_count": len(evidence),
                "evidence_summary": [self._summarize_evidence_item(e) for e in evidence[:10]]
                if evidence
                else [],
                "model_mode": detail.get("model_mode", "deterministic"),
                "stage_durations": detail.get("stage_durations", {}),
                "server_features": self.server_features,
                "agentic_trace": detail.get("agentic_trace", {}),
                "action_plan": detail.get("action_plan", []),
                "evidence_gaps": detail.get("evidence_gaps", []),
            }

            # Add conflict matrix if present
            if detail.get("conflict_matrix"):
                analyst_view["conflict_matrix"] = detail["conflict_matrix"]

            # Add explanation preview if present
            if detail.get("explanation"):
                analyst_view["explanation_preview"] = detail["explanation"].get("markdown", "")[
                    :500
                ]

            self._reporter.record_stage(
                stage_name="Fraud Analyst Assessment",
                status=200 if passed else 400,
                elapsed_ms=0,
                request_method="ANALYSIS",
                request_url="validate",
                response_body=analyst_view,
                response_status=200 if passed else 400,
                notes=notes,
            )

        self.results = {
            "scenario": self.scenario.value,
            "severity": severity,
            "severity_min": self.expectations.severity_min,
            "severity_pass": severity_pass,
            "recommendations_count": len(recommendations),
            "min_recommendations": self.expectations.min_recommendations,
            "recommendation_pass": recommendation_pass,
            "evidence_count": len(evidence),
            "evidence_pass": evidence_pass,
            "summary": summary,
        }

        return passed

    def validate_pattern_scores(self, detail: dict[str, Any]) -> bool:
        """Validate specific patterns triggered with correct scores.

        This is an optional deeper check for pattern-specific tests.
        """
        insight = detail.get("insight") or {}
        recommendations = detail.get("recommendations", [])
        summary = insight.get("summary", "")
        severity = insight.get("severity", "LOW")
        evidence_text = (
            f"{summary} "
            + " ".join(
                f"{(rec.get('payload') or {}).get('title', '')} "
                f"{(rec.get('payload') or {}).get('impact', '')}"
                for rec in recommendations
                if isinstance(rec, dict)
            )
        ).lower()

        passed = True
        notes: list[str] = []

        # Pattern-specific validation
        if self.scenario == FraudScenario.CARD_TESTING_PATTERN:
            card_testing_token = "card testing" in evidence_text or "card-testing" in evidence_text
            corroborating_signal = any(
                token in evidence_text for token in ("velocity", "decline", "merchant")
            )
            if card_testing_token and corroborating_signal:
                msg = "[PASS] Card testing pattern detected"
                notes.append(msg)
            else:
                msg = "[FAIL] Expected card testing pattern in summary"
                notes.append(msg)
                passed = False

        elif self.scenario == FraudScenario.VELOCITY_BURST:
            if "velocity" in evidence_text or "burst" in evidence_text:
                msg = "[PASS] Velocity burst pattern detected"
                notes.append(msg)
            else:
                msg = "[FAIL] Expected velocity burst pattern in summary"
                notes.append(msg)
                passed = False

        elif self.scenario == FraudScenario.CROSS_MERCHANT_SPREAD:
            if "merchant" in evidence_text:
                msg = "[PASS] Cross-merchant pattern detected"
                notes.append(msg)
            else:
                msg = "[FAIL] Expected cross-merchant pattern in summary"
                notes.append(msg)
                passed = False

        elif self.scenario == FraudScenario.HIGH_DECLINE_RATIO:
            if "decline" in evidence_text or "ratio" in evidence_text:
                msg = "[PASS] High decline ratio pattern detected"
                notes.append(msg)
            elif any(
                isinstance(rec, dict)
                and (rec.get("type") or rec.get("recommendation_type")) == "rule_candidate"
                for rec in recommendations
            ):
                msg = "[PASS] High decline ratio pattern inferred from recommendation profile"
                notes.append(msg)
            else:
                msg = "[FAIL] Expected decline ratio pattern in summary"
                notes.append(msg)
                passed = False

        elif self.scenario == FraudScenario.AMOUNT_ROUND_NUMBER:
            if "amount anomaly" in evidence_text and (
                "round number" in evidence_text or "$500" in evidence_text
            ):
                msg = "[PASS] Amount round-number anomaly detected"
                notes.append(msg)
            elif (
                severity_gte(severity, self.expectations.severity_min)
                and len(recommendations) >= self.expectations.min_recommendations
            ):
                msg = "[PASS] Amount round-number scenario inferred from severity + recommendations"
                notes.append(msg)
            else:
                msg = "[FAIL] Expected round-number amount anomaly in summary"
                notes.append(msg)
                passed = False

        elif self.scenario == FraudScenario.AMOUNT_HIGH:
            if "amount anomaly" in evidence_text and (
                "high amount" in evidence_text or "elevated" in evidence_text
            ):
                msg = "[PASS] High-amount anomaly detected"
                notes.append(msg)
            elif (
                severity_gte(severity, self.expectations.severity_min)
                and len(recommendations) >= self.expectations.min_recommendations
            ):
                msg = "[PASS] High-amount scenario inferred from severity + recommendations"
                notes.append(msg)
            else:
                msg = "[FAIL] Expected high-amount anomaly in summary"
                notes.append(msg)
                passed = False

        elif self.scenario == FraudScenario.TIME_UNUSUAL_HOUR:
            if "time anomaly" in evidence_text and (
                "unusual hour" in evidence_text or "3:00" in evidence_text
            ):
                msg = "[PASS] Unusual-hour anomaly detected"
                notes.append(msg)
            elif (
                severity_gte(severity, self.expectations.severity_min)
                and len(recommendations) >= self.expectations.min_recommendations
            ):
                msg = "[PASS] Unusual-hour scenario inferred from severity + recommendations"
                notes.append(msg)
            else:
                msg = "[FAIL] Expected unusual-hour anomaly in summary"
                notes.append(msg)
                passed = False

        elif self.scenario == FraudScenario.TIMEZONE_MISMATCH:
            if (
                "timezone mismatch" in evidence_text
                or ("time anomaly" in evidence_text and "vs" in evidence_text)
                or (
                    severity_gte(severity, "MEDIUM")
                    and any(
                        isinstance(rec, dict)
                        and (rec.get("type") or rec.get("recommendation_type"))
                        in {"review_priority", "case_action", "manual_review"}
                        for rec in recommendations
                    )
                )
            ):
                msg = "[PASS] Timezone mismatch detected"
                notes.append(msg)
            elif (
                severity_gte(severity, self.expectations.severity_min)
                and len(recommendations) >= self.expectations.min_recommendations
            ):
                msg = "[PASS] Timezone mismatch scenario inferred from severity + recommendations"
                notes.append(msg)
            else:
                msg = "[FAIL] Expected timezone mismatch in summary"
                notes.append(msg)
                passed = False

        elif self.scenario == FraudScenario.COUNTER_EVIDENCE_EXTENDED:
            if not self.server_features.get("counter_evidence_enabled", False):
                msg = "[PASS] Counter-evidence feature disabled; relaxed keyword validation"
                notes.append(msg)
            elif any(
                token in evidence_text for token in ("avs", "cvv", "tokenized", "counter-evidence")
            ):
                msg = "[PASS] Extended counter-evidence detected"
                notes.append(msg)
            else:
                msg = "[FAIL] Expected extended counter-evidence in summary"
                notes.append(msg)
                passed = False

        elif self.scenario == FraudScenario.CARD_TESTING_SEQUENCE:
            if (
                "card testing" in evidence_text
                and ("increasing" in evidence_text or "sequence" in evidence_text)
                or (
                    "velocity" in evidence_text
                    and severity_gte(severity, "MEDIUM")
                    and any(
                        isinstance(rec, dict)
                        and (rec.get("type") or rec.get("recommendation_type"))
                        in {"review_priority", "case_action", "manual_review"}
                        for rec in recommendations
                    )
                )
            ):
                msg = "[PASS] Card-testing sequence anomaly detected"
                notes.append(msg)
            else:
                msg = "[FAIL] Expected card-testing sequence evidence in summary"
                notes.append(msg)
                passed = False

        elif self.scenario == FraudScenario.LEGITIMATE_WITH_COUNTER_EVIDENCE:
            # Check for counter-evidence keywords
            if not self.server_features.get("counter_evidence_enabled", False):
                msg = "[PASS] Counter-evidence feature disabled; relaxed keyword validation"
                notes.append(msg)
            elif any(kw in evidence_text for kw in ["3ds", "trusted device", "authenticated"]):
                msg = "[PASS] Counter-evidence detected in summary"
                notes.append(msg)
            else:
                msg = "[FAIL] Expected counter-evidence keywords in summary"
                notes.append(msg)
                passed = False

        if self._reporter and notes:
            self._reporter.record_stage(
                stage_name="Validate Pattern Scores",
                status=200 if passed else 400,
                elapsed_ms=0,
                request_method="INTERNAL",
                request_url="validate_patterns",
                response_body={"summary": summary},
                response_status=200 if passed else 400,
                notes=notes,
            )

        for note in notes:
            self.log("VALIDATE_PATTERN", note)

        return passed

    def get_insights(self, transaction_id: str) -> dict[str, Any]:
        """Get insights for a transaction."""
        self.log("INSIGHTS", f"Fetching insights for {transaction_id}")

        url = f"{API_PREFIX}/transactions/{transaction_id}/insights"
        start = time.perf_counter()
        response = self.client.get(url)
        elapsed = (time.perf_counter() - start) * 1000

        self.log("INSIGHTS", f"HTTP {response.status_code} ({elapsed:.0f}ms)")

        if self._reporter:
            self._reporter.record_stage(
                stage_name="Get Transaction Insights",
                status=response.status_code,
                elapsed_ms=elapsed,
                request_method="GET",
                request_url=f"{BASE_URL}{url}",
                response_status=response.status_code,
                response_body=response.json() if response.status_code == 200 else None,
                error=response.text if response.status_code != 200 else None,
            )

        if response.status_code != 200:
            raise AssertionError(f"Get insights failed: {response.status_code}")

        return response.json()

    def get_worklist(self) -> dict[str, Any]:
        """Get the worklist of recommendations."""
        self.log("WORKLIST", "Fetching worklist")

        url = f"{API_PREFIX}/worklist/recommendations"
        start = time.perf_counter()
        response = self.client.get(url)
        elapsed = (time.perf_counter() - start) * 1000

        self.log("WORKLIST", f"HTTP {response.status_code} ({elapsed:.0f}ms)")

        if self._reporter:
            self._reporter.record_stage(
                stage_name="Get Worklist",
                status=response.status_code,
                elapsed_ms=elapsed,
                request_method="GET",
                request_url=f"{BASE_URL}{url}",
                response_status=response.status_code,
                response_body=response.json() if response.status_code == 200 else None,
                error=response.text if response.status_code != 200 else None,
            )

        if response.status_code != 200:
            raise AssertionError(f"Get worklist failed: {response.status_code}")

        return response.json()

    def acknowledge_recommendation(self, recommendation_id: str) -> dict[str, Any]:
        """Acknowledge a recommendation."""
        self.log("ACKNOWLEDGE", f"Acknowledging recommendation {recommendation_id}")

        url = f"{API_PREFIX}/worklist/recommendations/{recommendation_id}/acknowledge"
        req_body = {"action": "ACKNOWLEDGED", "comment": "E2E test scenario"}
        start = time.perf_counter()
        response = self.client.post(url, json=req_body)
        elapsed = (time.perf_counter() - start) * 1000

        self.log("ACKNOWLEDGE", f"HTTP {response.status_code} ({elapsed:.0f}ms)")

        if self._reporter:
            self._reporter.record_stage(
                stage_name="Acknowledge Recommendation",
                status=response.status_code,
                elapsed_ms=elapsed,
                request_method="POST",
                request_url=f"{BASE_URL}{url}",
                request_body=req_body,
                response_status=response.status_code,
                response_body=response.json() if response.status_code == 200 else None,
                error=response.text if response.status_code != 200 else None,
            )

        if response.status_code != 200:
            raise AssertionError(f"Acknowledge failed: {response.status_code}")

        return response.json()

    def run(self) -> bool:
        """Run the full scenario test with all 7 stages."""
        try:
            # Stage 1: Find transaction
            transaction_id = self.find_transaction()
            if not transaction_id:
                raise AssertionError(
                    "No matching transaction found. Re-seed scenarios with: "
                    "doppler run --config local -- uv run python scripts/seed_test_scenarios.py"
                )

            # Stage 2: Run investigation
            run_data = self.run_investigation(transaction_id)
            run_id = run_data["run_id"]

            # Stage 3: Get investigation detail
            detail = self.get_investigation_detail(run_id)

            # Stage 4: Get insights (separate endpoint)
            self.get_insights(transaction_id)

            # Stage 5: Get worklist
            worklist = self.get_worklist()
            recommendations = worklist.get("recommendations", [])

            # Stage 6: Acknowledge a recommendation (if any exist)
            if recommendations:
                # Find the recommendation for our transaction
                for rec in recommendations:
                    if rec.get("transaction_id") == transaction_id:
                        rec_id = rec.get("recommendation_id")
                        self.acknowledge_recommendation(rec_id)
                        self.log("ACKNOWLEDGE", f"Acknowledged {rec_id}")
                        break

            # Stage 7: Validate expectations
            passed = self.validate_expectations(detail)
            pattern_passed = self.validate_pattern_scores(detail)
            passed = passed and pattern_passed

            if passed:
                self.log("PASS", "[PASS] All expectations met")
            else:
                self.log("FAIL", "[FAIL] Some expectations not met")

            self.results.update(
                {
                    "scenario": self.scenario.value,
                    "passed": passed,
                    "pattern_passed": pattern_passed,
                    "run_investigation_ms": self.timings.get("run_investigation_ms"),
                    "get_investigation_detail_ms": self.timings.get("get_investigation_detail_ms"),
                }
            )
            SCENARIO_KPI_RESULTS[self.scenario.value] = dict(self.results)
            return passed

        except Exception as e:
            self.log("ERROR", f"[FAIL] Test failed: {e}")
            self.results.update(
                {
                    "scenario": self.scenario.value,
                    "passed": False,
                    "error": str(e),
                    "run_investigation_ms": self.timings.get("run_investigation_ms"),
                    "get_investigation_detail_ms": self.timings.get("get_investigation_detail_ms"),
                }
            )
            SCENARIO_KPI_RESULTS[self.scenario.value] = dict(self.results)
            return False


@pytest.mark.e2e
@pytest.mark.parametrize(
    "scenario",
    [
        FraudScenario.LIKELY_FRAUD,
        FraudScenario.LEGITIMATE,
    ],
)
def test_fraud_scenario(scenario: FraudScenario, e2e_reporter: E2EReporter):
    """Test fraud detection scenarios with expected outcomes."""
    runner = ScenarioTestRunner(scenario, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail(f"Scenario {scenario} failed expectations")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_card_testing_pattern(e2e_reporter: E2EReporter):
    """Test card testing pattern detection — multiple small declines at different merchants."""
    runner = ScenarioTestRunner(FraudScenario.CARD_TESTING_PATTERN, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Card testing pattern scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_velocity_burst(e2e_reporter: E2EReporter):
    """Test velocity burst detection — >10 tx in 1h triggers 0.9 score."""
    runner = ScenarioTestRunner(FraudScenario.VELOCITY_BURST, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Velocity burst scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_cross_merchant_spread(e2e_reporter: E2EReporter):
    """Test cross-merchant spread detection — >10 merchants in 24h triggers 0.8 score."""
    runner = ScenarioTestRunner(FraudScenario.CROSS_MERCHANT_SPREAD, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Cross merchant spread scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_high_decline_ratio(e2e_reporter: E2EReporter):
    """Test high decline ratio detection — >50% decline rate triggers 0.9 score."""
    runner = ScenarioTestRunner(FraudScenario.HIGH_DECLINE_RATIO, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("High decline ratio scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_legitimate_counter_evidence(e2e_reporter: E2EReporter):
    """Test legitimate transaction with counter-evidence — should downgrade severity."""
    runner = ScenarioTestRunner(
        FraudScenario.LEGITIMATE_WITH_COUNTER_EVIDENCE, reporter=e2e_reporter
    )
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Legitimate counter-evidence scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_likely_fraud(e2e_reporter: E2EReporter):
    """Test likely fraud detection — should flag MEDIUM severity."""
    runner = ScenarioTestRunner(FraudScenario.LIKELY_FRAUD, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Likely fraud scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_legitimate(e2e_reporter: E2EReporter):
    """Test legitimate transaction — should flag LOW severity, minimal alerts."""
    runner = ScenarioTestRunner(FraudScenario.LEGITIMATE, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Legitimate scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_approved_likely_fraud(e2e_reporter: E2EReporter):
    """Test approved transaction that is flagged as suspicious by the pipeline."""
    runner = ScenarioTestRunner(FraudScenario.APPROVED_LIKELY_FRAUD, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Approved likely fraud scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_edge_first_transaction(e2e_reporter: E2EReporter):
    """Test first transaction edge case."""
    runner = ScenarioTestRunner(FraudScenario.EDGE_FIRST_TRANSACTION, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Edge first transaction scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_edge_missing_data(e2e_reporter: E2EReporter):
    """Test missing optional fields edge case."""
    runner = ScenarioTestRunner(FraudScenario.EDGE_MISSING_DATA, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Edge missing data scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_amount_round_number(e2e_reporter: E2EReporter):
    """Test amount round-number anomaly scenario."""
    runner = ScenarioTestRunner(FraudScenario.AMOUNT_ROUND_NUMBER, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Amount round-number scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_amount_high(e2e_reporter: E2EReporter):
    """Test high-amount anomaly scenario."""
    runner = ScenarioTestRunner(FraudScenario.AMOUNT_HIGH, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Amount high scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_time_unusual_hour(e2e_reporter: E2EReporter):
    """Test unusual-hour time anomaly scenario."""
    runner = ScenarioTestRunner(FraudScenario.TIME_UNUSUAL_HOUR, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Time unusual-hour scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_timezone_mismatch(e2e_reporter: E2EReporter):
    """Test timezone mismatch scenario."""
    runner = ScenarioTestRunner(FraudScenario.TIMEZONE_MISMATCH, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Timezone mismatch scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_counter_evidence_extended(e2e_reporter: E2EReporter):
    """Test extended counter-evidence scenario."""
    runner = ScenarioTestRunner(FraudScenario.COUNTER_EVIDENCE_EXTENDED, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Extended counter-evidence scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_scenario_card_testing_sequence(e2e_reporter: E2EReporter):
    """Test card-testing sequence scenario."""
    runner = ScenarioTestRunner(FraudScenario.CARD_TESTING_SEQUENCE, reporter=e2e_reporter)
    try:
        passed = runner.run()
        if not passed:
            pytest.fail("Card testing sequence scenario failed")
    finally:
        runner.close()


@pytest.mark.e2e
def test_acceptance_kpi_gate(e2e_reporter: E2EReporter):
    """Evaluate suite-level acceptance KPIs for fraud-ops readiness."""
    if e2e_reporter:
        e2e_reporter.begin_scenario("Acceptance KPI Gate")

    expected = {scenario.value for scenario in FraudScenario}
    observed = set(SCENARIO_KPI_RESULTS)
    missing = sorted(expected - observed)
    kpis = compute_acceptance_kpis(SCENARIO_KPI_RESULTS)

    notes: list[str] = []
    passed = True
    if missing:
        passed = False
        notes.append(f"[FAIL] Missing scenario results: {', '.join(missing)}")

    failed_kpis: list[str] = []
    for name, metric in kpis.items():
        line = (
            f"{name}: value={metric['value']} target={metric['target']} "
            f"detail={metric['description']}"
        )
        if metric.get("pass"):
            notes.append(f"[PASS] {line}")
        else:
            passed = False
            failed_kpis.append(name)
            notes.append(f"[FAIL] {line}")

    if e2e_reporter:
        e2e_reporter.record_stage(
            stage_name="Evaluate Acceptance KPIs",
            status=200 if passed else 400,
            elapsed_ms=0,
            request_method="INTERNAL",
            request_url="acceptance-kpi-gate",
            response_status=200 if passed else 400,
            response_body={
                "expected_scenarios": sorted(expected),
                "observed_scenarios": sorted(observed),
                "missing_scenarios": missing,
                "kpis": kpis,
                "scenario_results": SCENARIO_KPI_RESULTS,
            },
            notes=notes,
        )

    assert not missing, f"Missing KPI scenario results: {', '.join(missing)}"
    assert not failed_kpis, f"Acceptance KPIs failed: {', '.join(failed_kpis)}"


@pytest.mark.e2e
def test_llm_hybrid_mode(e2e_reporter: E2EReporter):
    """Verify LLM hybrid mode is active when enabled."""
    if e2e_reporter:
        e2e_reporter.begin_scenario("LLM Hybrid Mode Check")
    client = httpx.Client(base_url=BASE_URL, timeout=TIMEOUT)
    txn_id = SEED_MANIFEST.get(FraudScenario.CARD_TESTING_PATTERN.value)
    if e2e_reporter:
        e2e_reporter.record_stage(
            stage_name="Find Transaction (Seed Manifest)",
            status=200 if txn_id else 400,
            elapsed_ms=0,
            request_method="INTERNAL",
            request_url=str(SEED_MANIFEST_PATH),
            response_status=200 if txn_id else 400,
            response_body={
                "scenario": FraudScenario.CARD_TESTING_PATTERN.value,
                "transaction_id": txn_id,
            },
            notes=(
                ["[PASS] Using seeded transaction for LLM hybrid check"]
                if txn_id
                else [
                    "[FAIL] Missing seeded transaction for LLM hybrid check; reseed scenarios first",
                ]
            ),
        )
    assert txn_id, (
        "Missing seed transaction for LLM check. Run: doppler run --config local -- "
        "uv run python scripts/seed_test_scenarios.py"
    )

    case_id = f"e2e-llm-{int(time.time() * 1000)}-{uuid4().hex[:8]}"
    run_request = {"transaction_id": txn_id, "mode": "quick", "case_id": case_id}

    # Run investigation (handle 409 = already investigated, fetch detail to check model_mode)
    start = time.perf_counter()
    response = client.post(
        f"{API_PREFIX}/investigations/run",
        json=run_request,
    )
    elapsed = (time.perf_counter() - start) * 1000

    run_id = None
    if response.status_code == 409:
        # Transaction already investigated — get run_id from conflict response
        body = response.json()
        errors = body.get("errors", {})
        detail = body.get("detail", {})
        run_id = (errors.get("run_id") if isinstance(errors, dict) else None) or (
            detail.get("run_id") if isinstance(detail, dict) else None
        )
        if e2e_reporter:
            e2e_reporter.record_stage(
                stage_name="Run Investigation (409 Conflict)",
                status=200,  # Treat as pass since we can proceed
                elapsed_ms=elapsed,
                request_method="POST",
                request_url=f"{BASE_URL}{API_PREFIX}/investigations/run",
                request_body=run_request,
                response_status=409,
                response_body=body,
                notes=["[PASS] Already investigated - reusing existing run_id"],
            )
    else:
        response.raise_for_status()
        data = response.json()
        run_id = data.get("run_id")
        if e2e_reporter:
            e2e_reporter.record_stage(
                stage_name="Run Investigation",
                status=response.status_code,
                elapsed_ms=elapsed,
                request_method="POST",
                request_url=f"{BASE_URL}{API_PREFIX}/investigations/run",
                request_body=run_request,
                response_status=response.status_code,
                response_body=data,
            )

    # Fetch detail to check model_mode
    if run_id:
        data: dict[str, Any] | None = None
        elapsed = 0.0
        attempts = 4
        detail_notes: list[str] = []
        for attempt in range(1, attempts + 1):
            start = time.perf_counter()
            response = client.get(f"{API_PREFIX}/investigations/{run_id}")
            elapsed = (time.perf_counter() - start) * 1000
            if response.status_code == 200:
                data = response.json()
                if attempt > 1:
                    detail_notes.append(
                        f"[PASS] Investigation detail available after {attempt} attempts"
                    )
                break
            if response.status_code == 404 and attempt < attempts:
                time.sleep(1.0)
                continue
            response.raise_for_status()

        if data is None:
            raise AssertionError(f"Investigation detail not available after {attempts} attempts")

        if e2e_reporter:
            e2e_reporter.record_stage(
                stage_name="Get Investigation Detail",
                status=200,
                elapsed_ms=elapsed,
                request_method="GET",
                request_url=f"{BASE_URL}{API_PREFIX}/investigations/{run_id}",
                response_status=200,
                response_body=data,
                notes=detail_notes,
            )
    else:
        # If no run_id, skip with scenario recorded
        client.close()
        raise AssertionError("Already investigated but no run_id in conflict response")

    # Check model_mode
    model_mode = data.get("model_mode")
    print(f"\n[LLM] Model mode: {model_mode}")

    # If OPS_AGENT_ENABLE_LLM_REASONING=true, should be "hybrid"
    # If disabled, should be "deterministic"
    # We don't fail on this, just log it
    assert model_mode in ("hybrid", "deterministic"), f"Unknown model_mode: {model_mode}"

    if e2e_reporter:
        e2e_reporter.record_stage(
            stage_name="Validate Model Mode",
            status=200,
            elapsed_ms=0,
            request_method="INTERNAL",
            request_url="validate",
            response_body={"model_mode": model_mode},
            notes=[f"[PASS] Model mode is {model_mode}"],
        )

    client.close()


@pytest.mark.e2e
def test_end_to_end_acknowledge_flow(e2e_reporter: E2EReporter):
    """Test full acknowledge flow from investigation to action."""
    if e2e_reporter:
        e2e_reporter.begin_scenario("End-to-End Acknowledge Flow")
    client = httpx.Client(base_url=BASE_URL, timeout=TIMEOUT)

    # Stage 1: Use deterministic seeded transaction
    txn_id = SEED_MANIFEST.get(FraudScenario.CARD_TESTING_PATTERN.value)
    if e2e_reporter:
        e2e_reporter.record_stage(
            stage_name="Find Transaction (Seed Manifest)",
            status=200 if txn_id else 400,
            elapsed_ms=0,
            request_method="INTERNAL",
            request_url=str(SEED_MANIFEST_PATH),
            response_status=200 if txn_id else 400,
            response_body={
                "scenario": FraudScenario.CARD_TESTING_PATTERN.value,
                "transaction_id": txn_id,
            },
            notes=(
                ["[PASS] Using seeded transaction for acknowledge flow"]
                if txn_id
                else ["[FAIL] Missing seed transaction for acknowledge flow"]
            ),
        )
    assert txn_id, (
        "Missing seed transaction for acknowledge flow. Run: doppler run --config local -- "
        "uv run python scripts/seed_test_scenarios.py"
    )

    case_id = f"e2e-ack-{int(time.time() * 1000)}-{uuid4().hex[:8]}"
    run_request = {"transaction_id": txn_id, "mode": "quick", "case_id": case_id}

    # Stage 2: Run investigation (handle 409 = already investigated, get existing run_id)
    start = time.perf_counter()
    response = client.post(
        f"{API_PREFIX}/investigations/run",
        json=run_request,
    )
    elapsed = (time.perf_counter() - start) * 1000

    run_id = None
    if response.status_code == 409:
        body = response.json()
        errors = body.get("errors", {})
        detail = body.get("detail", {})
        run_id = (errors.get("run_id") if isinstance(errors, dict) else None) or (
            detail.get("run_id") if isinstance(detail, dict) else None
        )
        if e2e_reporter:
            e2e_reporter.record_stage(
                stage_name="Run Investigation (409 Conflict)",
                status=200,
                elapsed_ms=elapsed,
                request_method="POST",
                request_url=f"{BASE_URL}{API_PREFIX}/investigations/run",
                request_body=run_request,
                response_status=409,
                response_body=body,
                notes=["[PASS] Already investigated - reusing existing run_id"],
            )
        if not run_id:
            raise AssertionError("Already investigated but no run_id in conflict response")
    else:
        response.raise_for_status()
        data = response.json()
        run_id = data.get("run_id")
        if e2e_reporter:
            e2e_reporter.record_stage(
                stage_name="Run Investigation",
                status=response.status_code,
                elapsed_ms=elapsed,
                request_method="POST",
                request_url=f"{BASE_URL}{API_PREFIX}/investigations/run",
                request_body=run_request,
                response_status=response.status_code,
                response_body=data,
            )

    # Stage 3: Get worklist
    start = time.perf_counter()
    response = client.get(f"{API_PREFIX}/worklist/recommendations")
    elapsed = (time.perf_counter() - start) * 1000
    response.raise_for_status()
    worklist = response.json()

    if e2e_reporter:
        e2e_reporter.record_stage(
            stage_name="Get Worklist",
            status=response.status_code,
            elapsed_ms=elapsed,
            request_method="GET",
            request_url=f"{BASE_URL}{API_PREFIX}/worklist/recommendations",
            response_status=response.status_code,
            response_body=worklist,
        )

    recommendations = worklist.get("recommendations", [])
    assert recommendations, "No recommendations to acknowledge"

    rec_id = recommendations[0]["recommendation_id"]

    # Stage 4: Acknowledge
    start = time.perf_counter()
    response = client.post(
        f"{API_PREFIX}/worklist/recommendations/{rec_id}/acknowledge",
        json={"action": "ACKNOWLEDGED", "comment": "E2E test acknowledge flow"},
    )
    elapsed = (time.perf_counter() - start) * 1000
    response.raise_for_status()
    ack_result = response.json()

    if e2e_reporter:
        e2e_reporter.record_stage(
            stage_name="Acknowledge Recommendation",
            status=response.status_code,
            elapsed_ms=elapsed,
            request_method="POST",
            request_url=f"{BASE_URL}{API_PREFIX}/worklist/recommendations/{rec_id}/acknowledge",
            request_body={"action": "ACKNOWLEDGED", "comment": "E2E test acknowledge flow"},
            response_status=response.status_code,
            response_body=ack_result,
            notes=[f"[PASS] Status is {ack_result.get('status')}"],
        )

    # Stage 5: Verify status changed
    assert ack_result["status"] == "ACKNOWLEDGED"

    if e2e_reporter:
        e2e_reporter.record_stage(
            stage_name="Validate Acknowledged Status",
            status=200,
            elapsed_ms=0,
            request_method="INTERNAL",
            request_url="validate",
            response_body={"status": ack_result["status"]},
            notes=["[PASS] Status is ACKNOWLEDGED"],
        )

    client.close()
